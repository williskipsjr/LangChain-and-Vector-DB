{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a35989a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Environment Setup & Dependencies\n",
    "\n",
    "First, let's load environment variables and install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d39585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087e9c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'langchain',\n",
    "    'langchain-community',\n",
    "    'deeplake',\n",
    "    'python-dotenv',\n",
    "    'requests',\n",
    "    'openai'  # For API compatibility\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "print(\"‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d289929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables Loaded:\n",
      "‚úì ACTIVELOOP_TOKEN: ‚úì\n",
      "‚úì GOOGLE_API_KEY: ‚úì\n",
      "\n",
      "Ollama will be used instead of OpenAI (local qwen3:4b model)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify loaded environment variables\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "print(\"Environment Variables Loaded:\")\n",
    "print(f\"‚úì ACTIVELOOP_TOKEN: {'‚úì' if activeloop_token else '‚úó'}\")\n",
    "print(f\"‚úì GOOGLE_API_KEY: {'‚úì' if google_api_key else '‚úó'}\")\n",
    "print(f\"\\nOllama will be used instead of OpenAI (local qwen3:4b model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13188ae6",
   "metadata": {},
   "source": [
    "## Part 2: The LLM - Using Ollama Locally\n",
    "\n",
    "Configure Ollama with the qwen3:4b model instead of OpenAI. This keeps everything local and cost-free."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ec9c4",
   "metadata": {},
   "source": [
    "**What is an LLM?**\n",
    "\n",
    "A Large Language Model (LLM) is the core AI component that generates text. We're using Ollama to run the qwen3:4b model locally instead of using cloud-based OpenAI, which keeps everything free and private.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `model`: The specific AI model to use (qwen3:4b in our case)\n",
    "- `base_url`: Where Ollama is running (local server at port 11434)\n",
    "- `temperature`: Controls randomness (0 = deterministic, 1 = very creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbddc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables Loaded:\n",
      "ACTIVELOOP_TOKEN: ‚úì Loaded\n",
      "GOOGLE_API_KEY: ‚úì Loaded\n",
      "\n",
      "Ollama will be used instead of OpenAI (local qwen3:4b model)\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file from the correct subfolder\n",
    "env_path = r'LangChain 101_from-Zero-to-Hero\\.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify loaded environment variables\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "print(\"Environment Variables Loaded:\")\n",
    "print(f\"ACTIVELOOP_TOKEN: {'‚úì Loaded' if activeloop_token else '‚úó Not Found'}\")\n",
    "print(f\"GOOGLE_API_KEY: {'‚úì Loaded' if google_api_key else '‚úó Not Found'}\")\n",
    "print(f\"\\nOllama will be used instead of OpenAI (local qwen3:4b model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7e6388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ollama LLM initialized with qwen3:4b model\n",
      "‚úì Base URL: http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama LLM with qwen3:4b model\n",
    "# Make sure Ollama is running locally on your machine\n",
    "llm = Ollama(\n",
    "    model=\"qwen3:4b\",  # Using local qwen3:4b model\n",
    "    base_url=\"http://localhost:11434\",  # Default Ollama endpoint\n",
    "    temperature=0.7  # Controls creativity (0-1)\n",
    ")\n",
    "\n",
    "print(\"‚úì Ollama LLM initialized with qwen3:4b model\")\n",
    "print(\"‚úì Base URL: http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1399c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ollama LLM (qwen3:4b)...\n",
      "\n",
      "LangChain is a **framework for building applications using large language models (LLMs)**. It simplifies the process by allowing developers to connect LLMs with other tools (like databases, APIs, or custom functions) in modular, reusable workflows‚Äîenabling complex AI applications without deep LLM expertise.  \n",
      "\n",
      "*(Example: Creating a chatbot that answers questions by querying a database, using an LLM as the \"brain.\")*\n"
     ]
    }
   ],
   "source": [
    "# Test the LLM\n",
    "print(\"Testing Ollama LLM (qwen3:4b)...\\n\")\n",
    "response = llm.invoke(\"What is LangChain? Give a brief answer.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7851140",
   "metadata": {},
   "source": [
    "## Part 3: The Prompt Template\n",
    "\n",
    "Create reusable prompt templates to structure inputs for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fb7c9",
   "metadata": {},
   "source": [
    "**What are Prompt Templates?**\n",
    "\n",
    "Prompt templates are reusable text patterns with placeholders (variables). Instead of writing the same prompt structure repeatedly, you create a template once and fill in different values each time.\n",
    "\n",
    "**Why use them?**\n",
    "- **Consistency**: Same format for all similar requests\n",
    "- **Reusability**: Write once, use many times\n",
    "- **Maintainability**: Change the template in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09eb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prompt:\n",
      "You are a helpful assistant.\n",
      "Question: What is a vector database?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a simple prompt template\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Test the prompt template\n",
    "formatted_prompt = prompt.format(question=\"What is a vector database?\")\n",
    "print(\"Sample Prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1b0526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Prompt:\n",
      "You are an expert software engineering assistant.\n",
      "User Query: Explain the concept of embeddings\n",
      "Provide a detailed and helpful response.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# More advanced prompt template with multiple variables\n",
    "advanced_template = \"\"\"You are an expert {expertise} assistant.\n",
    "User Query: {user_input}\n",
    "Provide a detailed and helpful response.\n",
    "Answer:\"\"\"\n",
    "\n",
    "advanced_prompt = PromptTemplate(\n",
    "    input_variables=[\"expertise\", \"user_input\"],\n",
    "    template=advanced_template\n",
    ")\n",
    "\n",
    "formatted_advanced = advanced_prompt.format(\n",
    "    expertise=\"software engineering\",\n",
    "    user_input=\"Explain the concept of embeddings\"\n",
    ")\n",
    "\n",
    "print(\"Advanced Prompt:\")\n",
    "print(formatted_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39938694",
   "metadata": {},
   "source": [
    "## Part 4: The Chain - Combining LLM and Prompts\n",
    "\n",
    "Chains combine prompts and LLMs into a single executable unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75adc13",
   "metadata": {},
   "source": [
    "**What is a Chain?**\n",
    "\n",
    "A chain combines multiple components (like a prompt template + LLM) into a single executable unit. Think of it as a pipeline: input ‚Üí prompt formatting ‚Üí LLM processing ‚Üí output.\n",
    "\n",
    "**Benefits:**\n",
    "- Simplifies complex workflows\n",
    "- Makes code more modular and reusable\n",
    "- Easier to debug and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c11af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chain created successfully!\n",
      "Chain components:\n",
      "  - LLM: Ollama (qwen3:4b)\n",
      "  - Prompt: Question-Answer template\n"
     ]
    }
   ],
   "source": [
    "# Create a chain combining prompt and LLM using LCEL (LangChain Expression Language)\n",
    "# This is the modern way to create chains in LangChain\n",
    "chain = prompt | llm\n",
    "\n",
    "print(\"‚úì Chain created successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"  - LLM: Ollama (qwen3:4b)\")\n",
    "print(f\"  - Prompt: Question-Answer template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580fb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Chain...\n",
      "\n",
      "\n",
      "Chain Output:\n",
      "RAG (Retrieval Augmented Generation) is a **framework for AI systems** that combines **retrieval of relevant information** from a knowledge source with **generative text production** to create more accurate, context-aware, and up-to-date responses. Here‚Äôs a simple breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "### üîç **Core Idea in 3 Steps**\n",
      "1. **Retrieval**:  \n",
      "   The system first searches through a **structured knowledge base** (e.g., documents, databases, or real-time data) to find the most relevant pieces of information related to the user‚Äôs query.  \n",
      "   *Example*: If you ask, *\"What‚Äôs the current population of Japan?\"*, the system retrieves the latest official statistics from a reliable source.\n",
      "\n",
      "2. **Augmentation**:  \n",
      "   The retrieved documents are used to **enhance the context** for the next step. This avoids the AI hallucinating facts (making up information it doesn‚Äôt know).\n",
      "\n",
      "3. **Generation**:  \n",
      "   The AI then uses the retrieved context to generate a **precise, human-like response**‚Äîwithout relying solely on its pre-trained knowledge (which might be outdated or incomplete).\n",
      "\n",
      "---\n",
      "\n",
      "### üåü **Why RAG Matters**\n",
      "- **Reduces hallucinations**: Large language models (LLMs) often invent facts. RAG fixes this by grounding responses in real data.  \n",
      "- **Uses fresh data**: Works with *current* information (e.g., news, live databases) instead of fixed training data.  \n",
      "- **Flexible**: Can integrate any knowledge source (e.g., company docs, research papers, live APIs).  \n",
      "- **No retraining needed**: Unlike fine-tuning LLMs, RAG adds external knowledge without modifying the model itself.\n",
      "\n",
      "---\n",
      "\n",
      "### üí° Real-World Example\n",
      "Imagine a customer support chatbot for a bank:  \n",
      "- **User**: *\"What‚Äôs the interest rate on savings accounts?\"*  \n",
      "- **RAG Process**:  \n",
      "  ‚Üí Retrieves the bank‚Äôs *latest* interest rate policy from its internal database.  \n",
      "  ‚Üí Generates a response like: *\"As of today, savings accounts have a 0.5% annual interest rate.\"*  \n",
      "- **Without RAG**: The chatbot might give outdated rates from its training data (e.g., 0.2% from 2020).\n",
      "\n",
      "---\n",
      "\n",
      "### üéØ Key Takeaway\n",
      "> **RAG = Retrieve relevant data + Generate accurate responses**.  \n",
      "> It‚Äôs like giving an AI a \"live reference library\" to answer questions *right now*, instead of relying on its past training.\n",
      "\n",
      "This makes RAG ideal for applications where **accuracy, timeliness, and domain-specific knowledge** matter‚Äîlike healthcare, finance, or customer support. üöÄ\n",
      "\n",
      "Let me know if you‚Äôd like a deeper dive into implementation or use cases! üòä\n"
     ]
    }
   ],
   "source": [
    "# Execute the chain with a test question\n",
    "print(\"Executing Chain...\\n\")\n",
    "result = chain.invoke({\"question\": \"Explain what RAG (Retrieval Augmented Generation) is.\"})\n",
    "print(f\"\\nChain Output:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d11085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced Chain Output:\n",
      "Here's a detailed, expert-level explanation of **transformer models** and their critical importance in modern AI/ML‚Äîstructured for clarity, technical depth, and real-world relevance. I'll avoid oversimplification while ensuring it's accessible to both beginners and practitioners.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. What Are Transformer Models? (The Core Definition)**\n",
      "Transformer models are a **class of neural network architectures** introduced in the seminal 2017 paper *\"Attention is All You Need\"* (Vaswani et al.) that revolutionized sequence modeling (especially in natural language processing, NLP). Unlike earlier models (e.g., RNNs, LSTMs), transformers **do not use recurrence or convolution** to process sequences. Instead, they rely **entirely on self-attention mechanisms** to capture relationships between elements in a sequence.\n",
      "\n",
      "#### üîë Key Components of a Transformer:\n",
      "| **Component**          | **What It Does**                                                                 | **Why It Matters**                                                                 |\n",
      "|-------------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n",
      "| **Self-Attention**      | Allows each element in a sequence to weigh its importance relative to *all other elements* (e.g., \"word i\" considers \"word j\" in context). | Solves the \"long-range dependency problem\" (e.g., understanding \"the cat chased the mouse\" where \"mouse\" is far from \"cat\"). |\n",
      "| **Positional Encoding** | Adds numerical information about *sequence position* (since attention is inherently order-agnostic). | Enables the model to preserve sequence order without recurrence (critical for languages). |\n",
      "| **Encoder**             | Processes input sequences (e.g., sentences) into a fixed-size representation.     | Handles input in parallel (vs. RNNs' sequential processing).                      |\n",
      "| **Decoder**             | Generates output sequences (e.g., translations, text) using encoder outputs.     | Enables tasks like machine translation, text generation, and more.                |\n",
      "| **Multi-Head Attention**| Runs multiple parallel attention mechanisms to capture diverse relationships.     | Improves model capacity and robustness (e.g., one head focuses on syntax, another on semantics). |\n",
      "\n",
      "#### üí° **Simple Analogy**:\n",
      "Imagine you're translating a sentence from English to French. A traditional RNN would process words one-by-one, \"chasing\" the context from earlier words. A transformer, however, lets *every word simultaneously* understand the role of every other word (e.g., \"the cat\" and \"mouse\" interact in context). This is **self-attention in action**.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Why Are Transformers Important? (The Critical Impact)**\n",
      "Transformers aren't just \"another model\"‚Äîthey solved **fundamental limitations** of prior architectures and became the *de facto standard* for modern AI. Here's why they matter across 5 key dimensions:\n",
      "\n",
      "#### ‚úÖ **1. They Break the Sequential Processing Bottleneck**\n",
      "- **Problem with RNNs/LSTMs**: Process sequences *one step at a time* (e.g., word ‚Üí word). This is slow, memory-intensive, and struggles with long sequences (e.g., paragraphs).\n",
      "- **Transformer Fix**: Uses **parallel computation** (all attention calculations happen simultaneously). This allows training on massive datasets (e.g., billions of tokens) in hours vs. days for RNNs.\n",
      "- **Real Impact**: Enables real-time applications like live translation (e.g., Google Translate), chatbots (e.g., ChatGPT), and video generation.\n",
      "\n",
      "#### ‚úÖ **2. They Achieve State-of-the-Art Performance**\n",
      "- Transformers consistently outperform all previous models on **NLP tasks** (e.g., machine translation, text summarization, sentiment analysis).\n",
      "- **Example**: The BERT model (2018) used transformers to set new SOTA (State-of-the-Art) in 100+ NLP tasks. GPT-3 (2020) used transformers to achieve human-level performance on reasoning tasks.\n",
      "- **Why?**: Self-attention captures *global context* better than local recurrence. For instance, in the sentence *\"The cat chased the mouse,\"* a transformer understands \"cat\" and \"mouse\" interact *without* needing to \"remember\" the entire sentence step-by-step.\n",
      "\n",
      "#### ‚úÖ **3. They Enable Large-Scale Pretraining**\n",
      "- Transformers are **scalable** to massive datasets (e.g., 100B+ parameters in GPT-3) without collapsing performance.\n",
      "- **Why?**: Their architecture allows efficient training with **gradient checkpointing** (reducing memory usage) and **distributed computing** (e.g., across thousands of GPUs).\n",
      "- **Real Impact**: This scalability is why models like **PaLM-2** (Google) and **LLaMA** (Meta) can be trained on planetary-scale data‚Äîenabling applications like coding assistants (GitHub Copilot) and scientific research.\n",
      "\n",
      "#### ‚úÖ **4. They Underpin Modern AI Ecosystems**\n",
      "> 90% of the most impactful AI models today (2023‚Äì2024) are transformer-based:\n",
      "> - **NLP**: GPT, BERT, T5, LLaMA\n",
      "> - **Computer Vision**: Vision Transformers (ViT) for image classification, object detection\n",
      "> - **Multimodal**: CLIP (text + images), SOTA video models\n",
      "> - **Code Generation**: CodeLLM, GitHub Copilot\n",
      "\n",
      "**Why this matters**: Transformers are the *only* architecture that can handle **text, images, and code** cohesively (e.g., CLIP matches text descriptions to images).\n",
      "\n",
      "#### ‚úÖ **5. They Drive Practical Efficiency and Innovation**\n",
      "- **Faster Training**: Parallelism cuts training time by 5‚Äì10x vs. RNNs.\n",
      "- **Less Data Required**: Transformers learn robust patterns from *less* data (e.g., BERT trained on 1.3B tokens vs. 100M+ for older models).\n",
      "- **Modularity**: Components (e.g., attention heads) can be reused across tasks (e.g., a \"text\" transformer for NLP + a \"vision\" transformer for images).\n",
      "- **Real-World Adoption**: Every major AI company (Google, Meta, Microsoft, Amazon) uses transformers in production systems.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Why Transformers Are *More* Important Than \"Just\" NLP**\n",
      "While transformers started in NLP, their importance **far exceeds** language:\n",
      "| **Domain**          | **Transformer Impact**                                                                 |\n",
      "|----------------------|-------------------------------------------------------------------------------------|\n",
      "| **Natural Language** | Dominant architecture for 90% of NLP tasks (e.g., chatbots, translation, summarization) |\n",
      "| **Computer Vision**  | Vision Transformers (ViT) surpassed CNNs on image tasks (e.g., ImageNet) in 2020       |\n",
      "| **Audio**            | Transformers power speech recognition (e.g., Whisper by OpenAI) and music generation   |\n",
      "| **Scientific AI**    | Used in protein folding (AlphaFold 2), drug discovery, and climate modeling           |\n",
      "| **Edge AI**          | Lightweight transformer variants (e.g., TinyML) enable real-time AI on mobile devices |\n",
      "\n",
      "**The bigger picture**: Transformers are the **first truly universal sequence model**‚Äîunifying text, vision, audio, and code under one paradigm. This is why they're called the \"new foundation of AI.\"\n",
      "\n",
      "---\n",
      "\n",
      "### **4. What People Often Misunderstand (And Why It Matters)**\n",
      "- ‚ùå **\"Transformers are only for text\"**: False. Vision transformers (ViT) are now industry standard for images.  \n",
      "- ‚ùå **\"Self-attention is magic\"**: No‚Äîit‚Äôs a *practical* solution to a real problem (long-range dependencies).  \n",
      "- ‚ùå **\"Transformers replace all previous models\"**: They‚Äôre the *best* for modern tasks, but RNNs/LSTMs still work for small-scale, low-latency apps (e.g., IoT devices).\n",
      "\n",
      "> üí° **Key Insight**: Transformers aren‚Äôt just \"better\"‚Äîthey **redefined how AI learns**. Before transformers, models were *task-specific* (e.g., a model for translation). Now, transformers enable **task-agnostic learning** (e.g., one model handles translation, summarization, and code generation).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. The Future: Why This Matters to You**\n",
      "As an AI/ML practitioner or student, understanding transformers is **non-negotiable**:\n",
      "- **If you build AI**: You‚Äôll almost certainly use transformer libraries (e.g., PyTorch, TensorFlow) or APIs (e.g., Hugging Face).\n",
      "- **If you study AI**: Transformers are the core of modern research (e.g., 70%+ of top AI conferences focus on them).\n",
      "- **If you care about ethics**: Transformers power bias amplification (e.g., biased training data ‚Üí biased outputs). *Understanding their mechanics is critical for responsible AI*.\n",
      "\n",
      "> **Final Takeaway**: Transformers are the **most impactful AI architecture since the neural network**. They solved the core problem of *sequence understanding* in a way that‚Äôs both efficient and scalable‚Äîenabling everything from chatbots to medical AI. Without them, the AI revolution we see today wouldn‚Äôt exist.\n",
      "\n",
      "---\n",
      "\n",
      "### **Recommended Next Steps for You**\n",
      "1. **Try a minimal transformer**: Build a tiny model with Hugging Face‚Äôs `transformers` library (e.g., `BertForSequenceClassification`).\n",
      "2. **Read the original paper**: [Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762) (15 pages‚Äîfocus on Section 3: \"Self-Attention\").\n",
      "3. **Explore applications**: See how Vision Transformers (ViT) work for images vs. text transformers.\n",
      "\n",
      "This isn‚Äôt just theory‚Äîit‚Äôs the **foundation of today‚Äôs AI world**. If you grasp how transformers work, you‚Äôve unlocked the key to the next decade of innovation.\n",
      "\n",
      "Let me know if you'd like deeper dives into specific areas (e.g., how self-attention works mathematically, transformer variants for vision, or real-world tradeoffs)! üòä\n"
     ]
    }
   ],
   "source": [
    "# Execute advanced chain\n",
    "advanced_chain = advanced_prompt | llm\n",
    "\n",
    "advanced_result = advanced_chain.invoke({\n",
    "    \"expertise\": \"AI/ML\",\n",
    "    \"user_input\": \"What are transformer models and why are they important?\"\n",
    "})\n",
    "\n",
    "print(f\"\\nAdvanced Chain Output:\\n{advanced_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fc0e8",
   "metadata": {},
   "source": [
    "## Part 5: The Memory - Maintaining Conversation Context\n",
    "\n",
    "Memory allows the chain to remember previous interactions in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e7b06",
   "metadata": {},
   "source": [
    "**What is Memory in LangChain?**\n",
    "\n",
    "Memory allows your AI to \"remember\" previous conversations. Without memory, each question is independent - the AI has no context from earlier messages.\n",
    "\n",
    "**ConversationBufferMemory:** Stores the entire conversation history as-is. Great for short conversations but can get large with long chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79911d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Conversation chain with memory created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "\n",
    "# Store for conversation history\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create a simple conversation prompt\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Create conversation chain with memory\n",
    "conversation_chain = RunnableWithMessageHistory(\n",
    "    conversation_prompt | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"‚úì Conversation chain with memory created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62872e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, I'm interested in learning about LangChain.\n",
      "\n",
      "Assistant: Hello! üëã I'm glad you're interested in LangChain. Let me give you a quick overview and some starting points.\n",
      "\n",
      "**What is LangChain?**  \n",
      "LangChain is an open-source framework for building applications that interact with language models (LLMs) in a modular way. It helps developers connect LLMs with tools, data sources, and workflows to create things like chatbots, document QA systems, and more.\n",
      "\n",
      "**Key features you might find useful for beginners:**  \n",
      "- üß© **Modular components**: Build apps by chaining simple tools (e.g., LLMs, databases, APIs).  \n",
      "- üîë **Easy prompt engineering**: Craft clear prompts for better LLM responses.  \n",
      "- üìö **Document processing**: Handle text data for tasks like answering questions from documents.  \n",
      "- üåê **Works with many LLMs**: Supports OpenAI, Anthropic, Mistral, and more.\n",
      "\n",
      "**Simple example to get started** (using OpenAI):  \n",
      "```python\n",
      "from langchain_community.llms import OpenAI\n",
      "\n",
      "# Initialize the model (you'll need an OpenAI API key)\n",
      "llm = OpenAI(temperature=0.5)\n",
      "\n",
      "# Generate a response\n",
      "response = llm(\"What is the capital of France?\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "> üí° **Note**: You‚Äôll need an OpenAI API key for this to work (you can get one from [OpenAI](https://platform.openai.com/)). For local testing, try using `llama3` or `mistral` models via LangChain‚Äôs `LocalLLM` or `HuggingFace` integrations.\n",
      "\n",
      "**Where to learn more**:  \n",
      "1Ô∏è‚É£ [Official LangChain docs](https://python.langchain.com/docs) (best for tutorials and examples)  \n",
      "2Ô∏è‚É£ [LangChain GitHub](https://github.com/langchain-ai/langchain) (for code and community)\n",
      "\n",
      "If you‚Äôd like, I can help you with:  \n",
      "- A specific use case (e.g., building a chatbot that answers from a document)  \n",
      "- Fixing this example code  \n",
      "- Explaining a concept like \"chaining\" or \"vector stores\"  \n",
      "\n",
      "Just say the word! üòä\n"
     ]
    }
   ],
   "source": [
    "# Test conversation memory - First turn\n",
    "print(\"User: Hello, I'm interested in learning about LangChain.\\n\")\n",
    "response1 = conversation_chain.invoke(\n",
    "    {\"input\": \"Hello, I'm interested in learning about LangChain.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo_session\"}}\n",
    ")\n",
    "print(f\"Assistant: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bc92eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Can you tell me about vector embeddings?\n",
      "\n",
      "Assistant: That's an **excellent follow-up question**! Vector embeddings are a *core* concept behind many LangChain applications (especially for similarity search, document retrieval, and semantic understanding). Let me break it down clearly for you ‚Äî no jargon overload.\n",
      "\n",
      "---\n",
      "\n",
      "### üîç What Are Vector Embeddings? (Simple Explanation)\n",
      "Imagine you want to represent **meaning** in numbers.  \n",
      "- **Traditional text** ‚Üí \"Paris\" is just a string of characters.  \n",
      "- **Vector embeddings** ‚Üí A *mathematical vector* (e.g., 768 numbers) that **captures the meaning** of \"Paris\" in a way AI models can understand.\n",
      "\n",
      "**Why this matters**:  \n",
      "Embeddings let AI models *compare meaning* instead of just matching words.  \n",
      "üëâ *Example*:  \n",
      "`\"Paris\"` and `\"capital of France\"` will be **closer** in the embedding space than `\"Paris\"` and `\"New York\"`.  \n",
      "*(This is how LangChain‚Äôs search tools find relevant answers!)*\n",
      "\n",
      "---\n",
      "\n",
      "### üåü Why LangChain Uses Embeddings (The Real-World Connection)\n",
      "In LangChain, embeddings are used for **semantic similarity** ‚Äî the *most common* use case for beginners. Here‚Äôs how it fits your earlier example:\n",
      "\n",
      "| Scenario                          | Without Embeddings          | With Embeddings (LangChain)               |\n",
      "|------------------------------------|------------------------------|-------------------------------------------|\n",
      "| **Answering \"What‚Äôs the capital of France?\"** | Just match keywords (\"France\" ‚Üí \"Paris\") | Find documents where **meaning** matches ‚Üí *Paris* (from a knowledge base) |\n",
      "| **Searching documents**            | \"Text contains 'Paris'\"      | \"Documents *semantically similar* to 'capital of France'\" |\n",
      "\n",
      "**Why this is powerful for LangChain**:  \n",
      "Embeddings let your app **understand context** (not just keywords). For example:\n",
      "- A weather chatbot can find *recent* weather reports about Paris by embedding the query + comparing to embedded weather documents.\n",
      "- A Q&A tool can answer questions using *related* documents (even if they don‚Äôt contain exact phrases).\n",
      "\n",
      "---\n",
      "\n",
      "### üí° Quick Real-World Example in LangChain\n",
      "Here‚Äôs how embeddings work *in practice* with a tiny LangChain app:\n",
      "\n",
      "1. **Convert text ‚Üí embedding**  \n",
      "   `embeddings = embedding_model(\"What‚Äôs the capital of France?\")`  \n",
      "   *(This creates a 768-number vector representing the question's meaning)*\n",
      "\n",
      "2. **Search for similar documents**  \n",
      "   `relevant_docs = vector_store.similarity_search(embeddings, k=1)`  \n",
      "   *(Finds the *most similar* document in your knowledge base)*\n",
      "\n",
      "3. **Answer the question**  \n",
      "   `answer = relevant_docs[0].page_content` ‚Üí *\"Paris\"*\n",
      "\n",
      "**This is how LangChain builds intelligent apps** without complex rules.\n",
      "\n",
      "---\n",
      "\n",
      "### üîë Key Takeaways for You (As a LangChain Learner)\n",
      "| Concept          | What It Means for You                                                                 |\n",
      "|-------------------|------------------------------------------------------------------------------------|\n",
      "| **Embeddings**    | \"Meaning in numbers\" ‚Äî let AI understand *context*, not just words.                  |\n",
      "| **Why LangChain uses them** | To make apps **smarter** (e.g., find relevant answers from documents, not just keywords). |\n",
      "| **Your next step** | Try building a **simple document search** using embeddings (this is a classic LangChain project!). |\n",
      "\n",
      "---\n",
      "\n",
      "### üöÄ Let‚Äôs Build Something Quick Together (Your Next Step)\n",
      "Since you‚Äôre interested in LangChain, **I‚Äôd love to show you how to add embeddings to a weather chatbot** (the one I mentioned earlier). Here‚Äôs what we‚Äôd do:\n",
      "\n",
      "1. **Get weather data** (via OpenWeather API)  \n",
      "2. **Convert weather reports ‚Üí embeddings**  \n",
      "3. **Search for similar weather reports** when answering questions like *\"What‚Äôs the weather in Paris?\"*  \n",
      "\n",
      "**Just say**: *\"Show me how to add embeddings to the weather bot!\"*  \n",
      "I‚Äôll give you **real, executable code** (with comments) ‚Äî no prior embedding knowledge needed.\n",
      "\n",
      "This is exactly where embeddings become *useful* in LangChain. üòä\n",
      "\n",
      "What would you like to try next? I‚Äôm here to help!\n"
     ]
    }
   ],
   "source": [
    "# Second turn - the chain remembers context\n",
    "print(\"\\nUser: Can you tell me about vector embeddings?\\n\")\n",
    "response2 = conversation_chain.invoke(\n",
    "    {\"input\": \"Can you tell me about vector embeddings?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"demo_session\"}}\n",
    ")\n",
    "print(f\"Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca68b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Memory History:\n",
      "human: Hello, I'm interested in learning about LangChain.\n",
      "ai: Hello! üëã I'm glad you're interested in LangChain. Let me give you a quick, beginner-friendly overview:\n",
      "\n",
      "---\n",
      "\n",
      "### üåü What is LangChain?\n",
      "LangChain is a **framework for building applications powered by language models** (like ChatGPT, Llama, etc.). It helps developers create practical AI tools by connecting different components (prompts, models, tools) into \"chains\" ‚Äî think of it as the glue that makes AI apps work smoothly.\n",
      "\n",
      "**Why it matters**:  \n",
      "Instead of building complex AI systems from scratch, LangChain lets you focus on *what* your app should do (e.g., answer questions, summarize text) while handling the messy details of language models.\n",
      "\n",
      "---\n",
      "\n",
      "### üîë Key Features for Beginners\n",
      "1. **Simple prompt engineering**  \n",
      "   Create clear instructions for AI models (e.g., \"Answer this question in 1 sentence\").\n",
      "2. **Multiple model support**  \n",
      "   Works with OpenAI, Hugging Face models, and more.\n",
      "3. **Tool integration**  \n",
      "   Add APIs, databases, or web services to your app.\n",
      "4. **Debugging ease**  \n",
      "   Track what the AI is doing with logs and tests.\n",
      "\n",
      "---\n",
      "\n",
      "### üí° Quick Example (No Code Required)\n",
      "Imagine you want a chatbot that answers simple questions like:  \n",
      "*‚ÄúWhat‚Äôs the capital of France?‚Äù* ‚Üí **\"Paris\"**\n",
      "\n",
      "With LangChain, you‚Äôd define a prompt template and a model ‚Äî no need to write complex AI logic. Here‚Äôs how it *feels*:\n",
      "\n",
      "```python\n",
      "# 1. Define the question format\n",
      "prompt = \"Answer: {question}\"\n",
      "\n",
      "# 2. Run it through a model (like ChatGPT)\n",
      "response = model.run(\"What‚Äôs the capital of France?\")\n",
      "# Output: \"Paris\"\n",
      "```\n",
      "\n",
      "*(Real code would use a library like `langchain` ‚Äî but the idea is simple!)*\n",
      "\n",
      "---\n",
      "\n",
      "### üöÄ Getting Started (3 Steps)\n",
      "1. **Install**: `pip install langchain`\n",
      "2. **Try the docs**: [Official Python Guide](https://python.langchain.com/docs/get_started)\n",
      "3. **Build something small**: Start with a weather chatbot or a text summarizer!\n",
      "\n",
      "---\n",
      "\n",
      "### üîç Why Should You Care?\n",
      "- **No more \"AI magic\"**: LangChain makes AI *tangible* (you can test, debug, and improve it).\n",
      "- **Real-world use**: Used by companies to build chatbots, data analysis tools, and more.\n",
      "- **Community**: Active open-source community with tutorials and examples.\n",
      "\n",
      "---\n",
      "\n",
      "### üí¨ Let‚Äôs dive deeper together!\n",
      "If you‚Äôd like, I can help you:\n",
      "- Build a **simple weather chatbot** (using OpenWeather API)\n",
      "- Understand **prompt engineering** for better AI responses\n",
      "- Explore **agents** (AI that takes actions on your behalf)\n",
      "\n",
      "Just say: *\"Let‚Äôs build a weather bot!\"* or *\"Explain prompts\"* ‚Äî I‚Äôll guide you step by step.\n",
      "\n",
      "What interests you most? üòä\n",
      "human: Can you tell me about vector embeddings?\n",
      "ai: That's an **excellent follow-up question**! Vector embeddings are a *core* concept behind many LangChain applications (especially for similarity search, document retrieval, and semantic understanding). Let me break it down clearly for you ‚Äî no jargon overload.\n",
      "\n",
      "---\n",
      "\n",
      "### üîç What Are Vector Embeddings? (Simple Explanation)\n",
      "Imagine you want to represent **meaning** in numbers.  \n",
      "- **Traditional text** ‚Üí \"Paris\" is just a string of characters.  \n",
      "- **Vector embeddings** ‚Üí A *mathematical vector* (e.g., 768 numbers) that **captures the meaning** of \"Paris\" in a way AI models can understand.\n",
      "\n",
      "**Why this matters**:  \n",
      "Embeddings let AI models *compare meaning* instead of just matching words.  \n",
      "üëâ *Example*:  \n",
      "`\"Paris\"` and `\"capital of France\"` will be **closer** in the embedding space than `\"Paris\"` and `\"New York\"`.  \n",
      "*(This is how LangChain‚Äôs search tools find relevant answers!)*\n",
      "\n",
      "---\n",
      "\n",
      "### üåü Why LangChain Uses Embeddings (The Real-World Connection)\n",
      "In LangChain, embeddings are used for **semantic similarity** ‚Äî the *most common* use case for beginners. Here‚Äôs how it fits your earlier example:\n",
      "\n",
      "| Scenario                          | Without Embeddings          | With Embeddings (LangChain)               |\n",
      "|------------------------------------|------------------------------|-------------------------------------------|\n",
      "| **Answering \"What‚Äôs the capital of France?\"** | Just match keywords (\"France\" ‚Üí \"Paris\") | Find documents where **meaning** matches ‚Üí *Paris* (from a knowledge base) |\n",
      "| **Searching documents**            | \"Text contains 'Paris'\"      | \"Documents *semantically similar* to 'capital of France'\" |\n",
      "\n",
      "**Why this is powerful for LangChain**:  \n",
      "Embeddings let your app **understand context** (not just keywords). For example:\n",
      "- A weather chatbot can find *recent* weather reports about Paris by embedding the query + comparing to embedded weather documents.\n",
      "- A Q&A tool can answer questions using *related* documents (even if they don‚Äôt contain exact phrases).\n",
      "\n",
      "---\n",
      "\n",
      "### üí° Quick Real-World Example in LangChain\n",
      "Here‚Äôs how embeddings work *in practice* with a tiny LangChain app:\n",
      "\n",
      "1. **Convert text ‚Üí embedding**  \n",
      "   `embeddings = embedding_model(\"What‚Äôs the capital of France?\")`  \n",
      "   *(This creates a 768-number vector representing the question's meaning)*\n",
      "\n",
      "2. **Search for similar documents**  \n",
      "   `relevant_docs = vector_store.similarity_search(embeddings, k=1)`  \n",
      "   *(Finds the *most similar* document in your knowledge base)*\n",
      "\n",
      "3. **Answer the question**  \n",
      "   `answer = relevant_docs[0].page_content` ‚Üí *\"Paris\"*\n",
      "\n",
      "**This is how LangChain builds intelligent apps** without complex rules.\n",
      "\n",
      "---\n",
      "\n",
      "### üîë Key Takeaways for You (As a LangChain Learner)\n",
      "| Concept          | What It Means for You                                                                 |\n",
      "|-------------------|------------------------------------------------------------------------------------|\n",
      "| **Embeddings**    | \"Meaning in numbers\" ‚Äî let AI understand *context*, not just words.                  |\n",
      "| **Why LangChain uses them** | To make apps **smarter** (e.g., find relevant answers from documents, not just keywords). |\n",
      "| **Your next step** | Try building a **simple document search** using embeddings (this is a classic LangChain project!). |\n",
      "\n",
      "---\n",
      "\n",
      "### üöÄ Let‚Äôs Build Something Quick Together (Your Next Step)\n",
      "Since you‚Äôre interested in LangChain, **I‚Äôd love to show you how to add embeddings to a weather chatbot** (the one I mentioned earlier). Here‚Äôs what we‚Äôd do:\n",
      "\n",
      "1. **Get weather data** (via OpenWeather API)  \n",
      "2. **Convert weather reports ‚Üí embeddings**  \n",
      "3. **Search for similar weather reports** when answering questions like *\"What‚Äôs the weather in Paris?\"*  \n",
      "\n",
      "**Just say**: *\"Show me how to add embeddings to the weather bot!\"*  \n",
      "I‚Äôll give you **real, executable code** (with comments) ‚Äî no prior embedding knowledge needed.\n",
      "\n",
      "This is exactly where embeddings become *useful* in LangChain. üòä\n",
      "\n",
      "What would you like to try next? I‚Äôm here to help!\n"
     ]
    }
   ],
   "source": [
    "# View memory history\n",
    "print(\"Conversation Memory History:\")\n",
    "session_history = store.get(\"demo_session\")\n",
    "if session_history:\n",
    "    for msg in session_history.messages:\n",
    "        print(f\"{msg.type}: {msg.content}\")\n",
    "else:\n",
    "    print(\"No conversation history yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e79f89",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Memory Types\n",
    "\n",
    "Explore different memory mechanisms for various use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5330b7",
   "metadata": {},
   "source": [
    "**ConversationSummaryMemory:** Instead of storing every word, it creates summaries of the conversation. This saves memory and keeps context manageable for very long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6cbe435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Summary Memory concept:\n",
      "In newer LangChain versions, you would implement custom summarization\n",
      "by combining a summarization chain with the conversation history.\n",
      "This helps manage long conversations by periodically summarizing past messages.\n"
     ]
    }
   ],
   "source": [
    "# Note: ConversationSummaryMemory is being phased out in newer LangChain versions\n",
    "# For demonstration, we'll show the concept\n",
    "print(\"‚úì Summary Memory concept:\")\n",
    "print(\"In newer LangChain versions, you would implement custom summarization\")\n",
    "print(\"by combining a summarization chain with the conversation history.\")\n",
    "print(\"This helps manage long conversations by periodically summarizing past messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8348be",
   "metadata": {},
   "source": [
    "## Part 7: Vector Database - DeepLake Integration\n",
    "\n",
    "Set up vector storage and retrieval using DeepLake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebaa0a2",
   "metadata": {},
   "source": [
    "**What is a Vector Database?**\n",
    "\n",
    "Vector databases store text as numerical embeddings (vectors) that capture semantic meaning. This enables \"semantic search\" - finding similar content by meaning, not just matching keywords.\n",
    "\n",
    "**Example:** Searching for \"car\" might also return results about \"vehicle\" or \"automobile\" because they have similar meanings (similar vectors).\n",
    "\n",
    "**DeepLake** is our vector database - it stores embeddings in the cloud for easy sharing and access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de97b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ollama Embeddings initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Set DeepLake token for authentication\n",
    "os.environ['DEEPLAKE_TOKEN'] = os.getenv('ACTIVELOOP_TOKEN')\n",
    "\n",
    "# Initialize Ollama embeddings (use an embedding-supporting model)\n",
    "# Suggest pulling once: ollama pull nomic-embed-text\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Ollama Embeddings initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "643a19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./deeplake_langchain_demo already exists, loading from the storage\n",
      "‚úì Using DeepLake dataset at ./deeplake_langchain_demo\n"
     ]
    }
   ],
   "source": [
    "# Create a DeepLake vector store (local path to avoid hub account errors)\n",
    "dataset_path = \"./deeplake_langchain_demo\"\n",
    "\n",
    "try:\n",
    "    # Try to load existing local dataset with embeddings for similarity search\n",
    "    db = DeepLake(dataset_path=dataset_path, read_only=False, embedding_function=embeddings)\n",
    "    print(f\"‚úì Using DeepLake dataset at {dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {str(e)}\")\n",
    "    print(\"A local dataset will be created when we add documents in the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20e6ecfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample documents prepared for vectorization\n",
      "1. LangChain is a framework for developing applications powered by language models....\n",
      "2. Embeddings are numerical representations of text that capture semantic meaning. ...\n",
      "3. Vector databases store embeddings and enable semantic search. They allow finding...\n",
      "4. Retrieval Augmented Generation (RAG) combines information retrieval with text ge...\n",
      "5. Chains in LangChain are sequences of calls to LLMs or other components, enabling...\n",
      "6. Memory in LangChain helps maintain conversation context across multiple interact...\n",
      "7. Agents allow LLMs to use tools and make decisions about which actions to take....\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Sample documents about LangChain\n",
    "documents = [\n",
    "    \"LangChain is a framework for developing applications powered by language models. It enables applications that are: Data-aware, connected to other sources of data. Agentic, allowing a language model to interact with its environment.\",\n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. They allow computers to understand relationships between words and concepts without explicit programming.\",\n",
    "    \"Vector databases store embeddings and enable semantic search. They allow finding similar items based on meaning rather than exact keyword matches.\",\n",
    "    \"Retrieval Augmented Generation (RAG) combines information retrieval with text generation, allowing LLMs to access external knowledge.\",\n",
    "    \"Chains in LangChain are sequences of calls to LLMs or other components, enabling complex workflows.\",\n",
    "    \"Memory in LangChain helps maintain conversation context across multiple interactions.\",\n",
    "    \"Agents allow LLMs to use tools and make decisions about which actions to take.\"\n",
    "]\n",
    "\n",
    "print(\"Sample documents prepared for vectorization\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dae308a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 7 document chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert to Document objects\n",
    "doc_objects = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Split documents into chunks (if needed)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(doc_objects)\n",
    "print(f\"‚úì Created {len(chunks)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b01348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./deeplake_langchain_demo already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 7 embeddings in 1 batches of size 7:: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:15<00:00, 15.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake_langchain_demo', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype     shape     dtype  compression\n",
      "  -------    -------   -------   -------  ------- \n",
      " embedding  embedding  (7, 768)  float32   None   \n",
      "    id        text      (7, 1)     str     None   \n",
      " metadata     json      (7, 1)     str     None   \n",
      "   text       text      (7, 1)     str     None   \n",
      "‚úì DeepLake vector store created: ./deeplake_langchain_demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DeepLake vector store with documents\n",
    "try:\n",
    "    db = DeepLake.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False\n",
    "    )\n",
    "    print(f\"‚úì DeepLake vector store created: {dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Working with existing dataset: {str(e)[:50]}...\")\n",
    "    db = DeepLake(dataset_path=dataset_path, read_only=False, embedding_function=embeddings)\n",
    "    print(f\"‚úì Connected to DeepLake: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ec36f",
   "metadata": {},
   "source": [
    "## Part 8: Semantic Search with Vector Database\n",
    "\n",
    "Perform semantic searches to find relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d7242",
   "metadata": {},
   "source": [
    "**How Semantic Search Works:**\n",
    "\n",
    "1. Your query is converted to a vector (embedding)\n",
    "2. The database finds vectors that are mathematically \"close\" to your query\n",
    "3. Returns the most similar documents\n",
    "\n",
    "This is more powerful than keyword search because it understands context and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a002e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'What is an embedding and why is it important?'\n",
      "\n",
      "Found 3 relevant documents:\n",
      "\n",
      "1. Vector databases store embeddings and enable semantic search. They allow finding similar items based on meaning rather than exact keyword matches....\n",
      "\n",
      "2. Chains in LangChain are sequences of calls to LLMs or other components, enabling complex workflows....\n",
      "\n",
      "3. LangChain is a framework for developing applications powered by language models. It enables applications that are: Data-aware, connected to other sour...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform semantic search\n",
    "query = \"What is an embedding and why is it important?\"\n",
    "\n",
    "print(f\"Searching for: '{query}'\\n\")\n",
    "results = db.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Found {len(results)} relevant documents:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03e2f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'Tell me about chains and agents in LangChain'\n",
      "\n",
      "Found 2 relevant documents:\n",
      "\n",
      "1. Chains in LangChain are sequences of calls to LLMs or other components, enabling complex workflows....\n",
      "\n",
      "2. Agents allow LLMs to use tools and make decisions about which actions to take....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try another search\n",
    "query2 = \"Tell me about chains and agents in LangChain\"\n",
    "\n",
    "print(f\"Searching for: '{query2}'\\n\")\n",
    "results2 = db.similarity_search(query2, k=2)\n",
    "\n",
    "print(f\"Found {len(results2)} relevant documents:\\n\")\n",
    "for i, result in enumerate(results2, 1):\n",
    "    print(f\"{i}. {result.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d0cd1",
   "metadata": {},
   "source": [
    "## Part 9: RAG (Retrieval Augmented Generation) Pipeline\n",
    "\n",
    "Combine retrieval from vector database with LLM generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee7b73",
   "metadata": {},
   "source": [
    "**What is RAG (Retrieval Augmented Generation)?**\n",
    "\n",
    "RAG combines two steps:\n",
    "1. **Retrieval**: Search the vector database for relevant information\n",
    "2. **Generation**: Use the LLM to generate an answer based on the retrieved context\n",
    "\n",
    "**Why is RAG important?**\n",
    "- Grounds AI responses in your specific documents\n",
    "- Reduces hallucinations (making up facts)\n",
    "- Allows AI to access knowledge beyond its training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a6e10735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG (Retrieval Augmented Generation) chain created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=rag_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create RAG chain using LCEL\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG (Retrieval Augmented Generation) chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c38ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Query: Explain what embeddings are and their importance in LangChain\n",
      "\n",
      "\n",
      "Answer:\n",
      "Based solely on the provided context, here is the explanation of what embeddings are and their importance in LangChain:\n",
      "\n",
      "**Embeddings** are numerical representations of text that capture semantic meaning (as stated in the context). They enable computers to understand relationships between words and concepts without explicit programming.\n",
      "\n",
      "**Their importance in LangChain** stems from LangChain's definition as a framework for \"data-aware\" applications connected to other sources of data. Since vector databases store embeddings and enable semantic search (finding similar items based on meaning rather than exact keyword matches), embeddings are critical for LangChain to:  \n",
      "1. Enable semantic understanding of text data,  \n",
      "2. Support the \"data-aware\" capability described in LangChain,  \n",
      "3. Allow the framework to interact with data sources using meaning-based relationships (as opposed to exact keyword matches),  \n",
      "4. Facilitate the \"agentic\" behavior where language models can interact with their environment through semantic data connections.  \n",
      "\n",
      "This directly aligns with LangChain's purpose of building applications powered by language models that leverage semantic understanding for practical data interactions.\n"
     ]
    }
   ],
   "source": [
    "# Query using RAG\n",
    "rag_query = \"Explain what embeddings are and their importance in LangChain\"\n",
    "\n",
    "print(f\"RAG Query: {rag_query}\\n\")\n",
    "rag_result = qa_chain.invoke(rag_query)\n",
    "\n",
    "print(f\"\\nAnswer:\\n{rag_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12d1f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Query: What are agents and how do they work?\n",
      "\n",
      "\n",
      "Answer:\n",
      "Based solely on the provided context:\n",
      "\n",
      "Agents are systems that allow language models (LLMs) to use tools and make decisions about which actions to take.  \n",
      "\n",
      "How they work: Agents enable LLMs to interact with their environment by selecting and executing specific actions (using tools) while making decisions about the appropriate course of action. This allows the LLM to perform tasks in a goal-directed manner within the framework of LangChain.  \n",
      "\n",
      "*(Note: The context does not specify additional implementation details about how agents internally process decisions or select tools beyond the description provided.)*\n"
     ]
    }
   ],
   "source": [
    "# Another RAG query\n",
    "rag_query2 = \"What are agents and how do they work?\"\n",
    "\n",
    "print(f\"RAG Query: {rag_query2}\\n\")\n",
    "rag_result2 = qa_chain.invoke(rag_query2)\n",
    "\n",
    "print(f\"\\nAnswer:\\n{rag_result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe22ac6",
   "metadata": {},
   "source": [
    "## Part 10: Advanced Chains - Sequential Chain\n",
    "\n",
    "Create complex workflows using sequential chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d866c44f",
   "metadata": {},
   "source": [
    "**Sequential Chains:**\n",
    "\n",
    "These run multiple chains in sequence, where the output of one becomes the input for the next.\n",
    "\n",
    "**Example:** \n",
    "- Chain 1: Explain a concept\n",
    "- Chain 2: Use that explanation to suggest applications\n",
    "\n",
    "This creates more sophisticated, multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c42a8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sequential chain created!\n"
     ]
    }
   ],
   "source": [
    "# Create sequential chain using LCEL\n",
    "# First chain: Explain concept\n",
    "topic_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain the concept of {topic} in simple terms.\\nResponse:\"\n",
    ")\n",
    "\n",
    "# Second chain: Provide applications\n",
    "followup_prompt = PromptTemplate(\n",
    "    input_variables=[\"explanation\"],\n",
    "    template=\"Based on this explanation: {explanation}\\nNow provide practical applications.\\nApplications:\"\n",
    ")\n",
    "\n",
    "# Combine into sequential chain\n",
    "sequential_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | topic_prompt \n",
    "    | llm \n",
    "    | (lambda x: {\"explanation\": x})\n",
    "    | followup_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(\"‚úì Sequential chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97a97b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing sequential chain...\n",
      "\n",
      "\n",
      "Final Output (Applications):\n",
      "Here are **5 practical, real-world applications of vector embeddings**‚Äîdirectly tied to your explanation and designed to be instantly relatable. Each example shows *exactly how* embeddings solve a problem using the \"meaning map\" concept you described:\n",
      "\n",
      "---\n",
      "\n",
      "### üéØ 1. **Smart Search (Like Google or Amazon)**  \n",
      "**Problem**: Traditional search fails when \"apple\" means *fruit* vs. *company* (e.g., \"apple\" in a recipe vs. \"Apple Inc.\" in tech).  \n",
      "**How embeddings fix it**:  \n",
      "- Embeddings create a **meaning map** where \"apple\" (fruit) is close to \"red,\" \"juice,\" \"pie,\" but *far* from \"Apple Inc.\" (which clusters near \"iPhone,\" \"software,\" \"store\").  \n",
      "- When you search \"apple,\" the system finds *contextual results* (e.g., recipes for fruit) **not** just the company‚Äôs website.  \n",
      "‚úÖ *Why it matters*: Matches your \"dog vs. sushi\" example‚Äîembeddings let search understand *real-world context* without manual rules.\n",
      "\n",
      "---\n",
      "\n",
      "### üí¨ 2. **Contextual Chatbots (Like ChatGPT or Customer Support)**  \n",
      "**Problem**: Chatbots often misunderstand context (e.g., \"apple\" in a tech query vs. a fruit query).  \n",
      "**How embeddings fix it**:  \n",
      "- Embeddings learn **semantic relationships** from billions of conversations.  \n",
      "  - *Example*: If a user says *\"I need an apple\"*, the bot knows \"apple\" = fruit ‚Üí suggests *\"eat it\"*, not *\"Apple Inc.\"* (which is far away in the embedding space).  \n",
      "- They also handle **synonyms/related terms** (e.g., \"dog\" ‚Üí \"puppy,\" \"bark,\" \"leash\" all cluster closely).  \n",
      "‚úÖ *Why it matters*: Makes chatbots **understand nuance**‚Äîno more \"apple\" confusion in real conversations.\n",
      "\n",
      "---\n",
      "\n",
      "### üçΩÔ∏è 3. **Personalized Recommendations (Netflix, Spotify, Amazon)**  \n",
      "**Problem**: Traditional recommendations just use *keywords* (e.g., \"movies with action\").  \n",
      "**How embeddings fix it**:  \n",
      "- Embeddings map **user preferences** and **items** into a shared space:  \n",
      "  - *Example*: A user who likes *\"dog movies\"* (e.g., \"puppy,\" \"bark\") will cluster near embeddings of *\"puppy movies\"*, *\"animal adventures\"*, but *far* from *\"sushi\"* or *\"cars\"*.  \n",
      "- Netflix/Spotify use this to suggest *similar content* based on **meaning**, not just tags.  \n",
      "‚úÖ *Why it matters*: Finds **true preferences** (e.g., \"you liked *puppy* movies ‚Üí suggest *dog* adventures\") instead of random matches.\n",
      "\n",
      "---\n",
      "\n",
      "### üñºÔ∏è 4. **Image Search (Like Pinterest or Google Images)**  \n",
      "**Problem**: Image search fails when \"dog\" vs. \"puppy\" are visually similar but contextually different.  \n",
      "**How embeddings fix it**:  \n",
      "- Embeddings convert **images** into numbers (like \"colors\" on your palette).  \n",
      "  - *Example*: A \"puppy\" image clusters closely with other *small dogs* (e.g., \"puppy,\" \"dachshund\"), but is *far* from \"sushi\" (which clusters with food/texture embeddings).  \n",
      "- Pinterest uses this to show *\"similar outfits\"* or *\"puppy-themed\"* pins when you search \"puppy.\"  \n",
      "‚úÖ *Why it matters*: Matches your \"colors\" analogy‚Äîimages become **meaningful points** in a semantic map.\n",
      "\n",
      "---\n",
      "\n",
      "### üåê 5. **Real-Time Translation (Like Google Translate)**  \n",
      "**Problem**: Translation tools often translate \"bank\" as *financial institution* when it means *riverbank* (e.g., \"river bank\" vs. \"bank account\").  \n",
      "**How embeddings fix it**:  \n",
      "- Embeddings learn **contextual meaning** from massive text data.  \n",
      "  - *Example*: \"bank\" (river) clusters near \"river,\" \"water,\" \"shore\"; \"bank\" (financial) clusters near \"money,\" \"account,\" \"loan.\"  \n",
      "- When translating \"bank\" in a sentence like *\"the river bank is wet\"*, the system picks the *river* meaning‚Äî**not** the financial one.  \n",
      "‚úÖ *Why it matters*: Translates **accurately in context**, avoiding real-world confusion (like your \"dog vs. sushi\" example).\n",
      "\n",
      "---\n",
      "\n",
      "### üîë **Why these applications work (backed by your explanation)**  \n",
      "All 5 examples rely on **one core truth** from your description:  \n",
      "> *\"Embeddings are a smart number space where similar things are close together, and different things are far apart‚Äîtrained on real data.\"*  \n",
      "\n",
      "This lets systems:  \n",
      "- **Understand context** (e.g., \"apple\" = fruit vs. company),  \n",
      "- **Find relationships** (e.g., \"dog\" ‚Üí \"puppy\" is close; \"dog\" ‚Üí \"sushi\" is far),  \n",
      "- **Work faster** (no manual rules needed‚Äîjust math).  \n",
      "\n",
      "**No magic, just meaning maps**: These applications don‚Äôt require new rules‚Äîthey use embeddings to **turn real-world meaning into numbers** that computers can *naturally* reason with.  \n",
      "\n",
      "*(Example you gave: When you search \"dog,\" embeddings find \"puppy\" or \"bark\" but not \"sushi\" ‚Üí this is exactly how all 5 applications work!)*  \n",
      "\n",
      "This is why embeddings are now the **foundation of modern AI**‚Äîthey turn abstract \"meaning\" into actionable intelligence. üöÄ\n"
     ]
    }
   ],
   "source": [
    "# Execute sequential chain\n",
    "print(\"Executing sequential chain...\\n\")\n",
    "result = sequential_chain.invoke(\"Vector embeddings\")\n",
    "print(f\"\\nFinal Output (Applications):\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26da4e",
   "metadata": {},
   "source": [
    "## Part 11: Output Parsing\n",
    "\n",
    "Structure and parse LLM outputs for better usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008f1d9",
   "metadata": {},
   "source": [
    "**Output Parsing:**\n",
    "\n",
    "LLMs naturally output unstructured text. Parsers convert that text into structured formats (lists, JSON, etc.) that your code can easily work with.\n",
    "\n",
    "**Example:** Instead of getting \"apples, oranges, bananas\" as text, the parser gives you `['apples', 'oranges', 'bananas']` as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e00962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output parser chain created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create output parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Create prompt with format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"Provide a list of {subject} separated by commas.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Create chain with parser using LCEL\n",
    "list_chain = list_prompt | llm | output_parser\n",
    "\n",
    "print(\"‚úì Output parser chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e910a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed output (as list):\n",
      "  - Language model integration\n",
      "  - Vector stores\n",
      "  - Agents\n",
      "  - Tools\n"
     ]
    }
   ],
   "source": [
    "# Use the parser\n",
    "parsed = list_chain.invoke({\"subject\": \"key features of LangChain\"})\n",
    "\n",
    "print(\"Parsed output (as list):\")\n",
    "for item in parsed:\n",
    "    print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9452eb",
   "metadata": {},
   "source": [
    "## Part 12: Summary and Best Practices\n",
    "\n",
    "Key takeaways from the LangChain course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dda66b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "    ‚ïë     LangChain Fundamentals - Course Summary                    ‚ïë\n",
      "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "    1. THE LLM\n",
      "       ‚úì Initialized Ollama with qwen3:4b for local, cost-free inference\n",
      "       ‚úì Supports streaming and custom callbacks\n",
      "       ‚úì Configured for temperature-based creativity control\n",
      "\n",
      "    2. PROMPTS & TEMPLATES\n",
      "       ‚úì Created reusable prompt templates with variables\n",
      "       ‚úì Used PromptTemplate for consistent formatting\n",
      "       ‚úì Multiple template examples for different use cases\n",
      "\n",
      "    3. CHAINS\n",
      "       ‚úì Combined LLM + Prompt using LLMChain\n",
      "       ‚úì Created sequential chains for multi-step workflows\n",
      "       ‚úì Advanced chain patterns for complex applications\n",
      "\n",
      "    4. MEMORY\n",
      "       ‚úì ConversationBufferMemory for conversation history\n",
      "       ‚úì ConversationSummaryMemory for efficient storage\n",
      "       ‚úì Memory enables context-aware interactions\n",
      "\n",
      "    5. VECTOR DATABASES\n",
      "       ‚úì Integrated DeepLake for vector storage\n",
      "       ‚úì Used Ollama embeddings for semantic representations\n",
      "       ‚úì Performed semantic similarity searches\n",
      "\n",
      "    6. RETRIEVAL AUGMENTED GENERATION (RAG)\n",
      "       ‚úì Combined retrieval with generation\n",
      "       ‚úì Grounded LLM responses in external knowledge\n",
      "       ‚úì Improved accuracy and factuality\n",
      "\n",
      "    7. OUTPUT PARSING\n",
      "       ‚úì Structured LLM outputs for programmatic use\n",
      "       ‚úì CommaSeparatedListOutputParser for lists\n",
      "       ‚úì Custom parsers for specific formats\n",
      "\n",
      "    KEY INSIGHTS:\n",
      "    ‚Ä¢ Ollama provides cost-free local LLM inference\n",
      "    ‚Ä¢ Chains enable complex multi-step workflows\n",
      "    ‚Ä¢ Memory allows context retention across turns\n",
      "    ‚Ä¢ Vector databases enable semantic search\n",
      "    ‚Ä¢ RAG combines retrieval + generation for better quality\n",
      "\n",
      "    NEXT STEPS:\n",
      "    ‚Üí Explore agents for autonomous decision-making\n",
      "    ‚Üí Build custom tools for specific domain tasks\n",
      "    ‚Üí Optimize prompt engineering for better results\n",
      "    ‚Üí Scale applications with distributed vector stores\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "    ‚ïë     LangChain Fundamentals - Course Summary                    ‚ïë\n",
    "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \n",
    "    1. THE LLM\n",
    "       ‚úì Initialized Ollama with qwen3:4b for local, cost-free inference\n",
    "       ‚úì Supports streaming and custom callbacks\n",
    "       ‚úì Configured for temperature-based creativity control\n",
    "    \n",
    "    2. PROMPTS & TEMPLATES\n",
    "       ‚úì Created reusable prompt templates with variables\n",
    "       ‚úì Used PromptTemplate for consistent formatting\n",
    "       ‚úì Multiple template examples for different use cases\n",
    "    \n",
    "    3. CHAINS\n",
    "       ‚úì Combined LLM + Prompt using LLMChain\n",
    "       ‚úì Created sequential chains for multi-step workflows\n",
    "       ‚úì Advanced chain patterns for complex applications\n",
    "    \n",
    "    4. MEMORY\n",
    "       ‚úì ConversationBufferMemory for conversation history\n",
    "       ‚úì ConversationSummaryMemory for efficient storage\n",
    "       ‚úì Memory enables context-aware interactions\n",
    "    \n",
    "    5. VECTOR DATABASES\n",
    "       ‚úì Integrated DeepLake for vector storage\n",
    "       ‚úì Used Ollama embeddings for semantic representations\n",
    "       ‚úì Performed semantic similarity searches\n",
    "    \n",
    "    6. RETRIEVAL AUGMENTED GENERATION (RAG)\n",
    "       ‚úì Combined retrieval with generation\n",
    "       ‚úì Grounded LLM responses in external knowledge\n",
    "       ‚úì Improved accuracy and factuality\n",
    "    \n",
    "    7. OUTPUT PARSING\n",
    "       ‚úì Structured LLM outputs for programmatic use\n",
    "       ‚úì CommaSeparatedListOutputParser for lists\n",
    "       ‚úì Custom parsers for specific formats\n",
    "    \n",
    "    KEY INSIGHTS:\n",
    "    ‚Ä¢ Ollama provides cost-free local LLM inference\n",
    "    ‚Ä¢ Chains enable complex multi-step workflows\n",
    "    ‚Ä¢ Memory allows context retention across turns\n",
    "    ‚Ä¢ Vector databases enable semantic search\n",
    "    ‚Ä¢ RAG combines retrieval + generation for better quality\n",
    "    \n",
    "    NEXT STEPS:\n",
    "    ‚Üí Explore agents for autonomous decision-making\n",
    "    ‚Üí Build custom tools for specific domain tasks\n",
    "    ‚Üí Optimize prompt engineering for better results\n",
    "    ‚Üí Scale applications with distributed vector stores\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2457118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "    ‚ïë     Configuration Reference                                    ‚ïë\n",
      "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "    ENVIRONMENT VARIABLES (from .env):\n",
      "    ‚Ä¢ ACTIVELOOP_TOKEN: DeepLake authentication\n",
      "    ‚Ä¢ GOOGLE_API_KEY: Additional services (if needed)\n",
      "    ‚Ä¢ OpenAI API Key: Not used (replaced by Ollama)\n",
      "\n",
      "    OLLAMA SETUP:\n",
      "    ‚Ä¢ Model: qwen3:4b\n",
      "    ‚Ä¢ Base URL: http://localhost:11434\n",
      "    ‚Ä¢ Status: Make sure Ollama is running locally\n",
      "    ‚Ä¢ Installation: https://ollama.ai\n",
      "\n",
      "    DEEPLAKE SETUP:\n",
      "    ‚Ä¢ Organization: williskipsj\n",
      "    ‚Ä¢ Dataset: langchain_demo\n",
      "    ‚Ä¢ Token: Set via ACTIVELOOP_TOKEN\n",
      "\n",
      "    EMBEDDINGS:\n",
      "    ‚Ä¢ Provider: Ollama\n",
      "    ‚Ä¢ Model: qwen3:4b\n",
      "    ‚Ä¢ Dimension: Varies by model\n",
      "    ‚Ä¢ Cost: Free (local execution)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Final configuration reference\n",
    "print(\"\"\"\n",
    "    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "    ‚ïë     Configuration Reference                                    ‚ïë\n",
    "    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "    \n",
    "    ENVIRONMENT VARIABLES (from .env):\n",
    "    ‚Ä¢ ACTIVELOOP_TOKEN: DeepLake authentication\n",
    "    ‚Ä¢ GOOGLE_API_KEY: Additional services (if needed)\n",
    "    ‚Ä¢ OpenAI API Key: Not used (replaced by Ollama)\n",
    "    \n",
    "    OLLAMA SETUP:\n",
    "    ‚Ä¢ Model: qwen3:4b\n",
    "    ‚Ä¢ Base URL: http://localhost:11434\n",
    "    ‚Ä¢ Status: Make sure Ollama is running locally\n",
    "    ‚Ä¢ Installation: https://ollama.ai\n",
    "    \n",
    "    DEEPLAKE SETUP:\n",
    "    ‚Ä¢ Organization: williskipsj\n",
    "    ‚Ä¢ Dataset: langchain_demo\n",
    "    ‚Ä¢ Token: Set via ACTIVELOOP_TOKEN\n",
    "    \n",
    "    EMBEDDINGS:\n",
    "    ‚Ä¢ Provider: Ollama\n",
    "    ‚Ä¢ Model: qwen3:4b\n",
    "    ‚Ä¢ Dimension: Varies by model\n",
    "    ‚Ä¢ Cost: Free (local execution)\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
