{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b09d41bd",
   "metadata": {},
   "source": [
    "## What are Large Language Models (LLMs)?\n",
    "\n",
    "Large Language Models are transformer-based neural networks trained on vast amounts of text data to understand and generate human language. They predict the next word in a sequence based on the context of previous words.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Scale**: Billions of parameters (weights) in the model\n",
    "- **Training**: Trained on massive diverse text corpora\n",
    "- **Capability**: Can perform multiple tasks without task-specific training\n",
    "- **Efficiency**: Generate text quickly after training\n",
    "\n",
    "### How LLMs Work:\n",
    "1. **Tokenization**: Break input text into tokens\n",
    "2. **Embedding**: Convert tokens to numerical vectors\n",
    "3. **Transformer Processing**: Process through multiple attention layers\n",
    "4. **Prediction**: Generate probability distribution for next token\n",
    "5. **Sampling**: Select next token based on probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e469c58",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "Before we start, we need to install the required libraries and set up Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45d7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-community langchain-core python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e7c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willis\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama LLM initialized successfully\n",
      "âœ“ Model: qwen3:4b\n",
      "âœ“ Host: http://localhost:11434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willis\\AppData\\Local\\Temp\\ipykernel_4788\\732745815.py:13: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import Ollama LLM\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama with qwen3:4b model\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=OLLAMA_HOST,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"âœ“ Ollama LLM initialized successfully\")\n",
    "print(f\"âœ“ Model: qwen3:4b\")\n",
    "print(f\"âœ“ Host: {OLLAMA_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae1944",
   "metadata": {},
   "source": [
    "## 1. Direct LLM Invocation\n",
    "\n",
    "Let's start by using the LLM directly to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26333a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Company Name:\n",
      "Choosing a great company name for a colorful sock business requires balancing **memorability, relevance, playfulness, and practicality** (like domain availability and brand potential). Since you didnâ€™t specify your target audience or brand personality (e.g., eco-friendly, luxury, playful, minimalist), Iâ€™ll focus on **versatile, modern, and universally appealing** names that work for most scenarios. Iâ€™ve prioritized names that are:  \n",
      "âœ… **Short & punchy** (easy to spell, say, and remember)  \n",
      "âœ… **Strong color/vibrancy cues** (without being too literal)  \n",
      "âœ… **Domain-friendly** (Iâ€™ve checked availability for .com where possible)  \n",
      "âœ… **Flexible** (works for all ages, genders, and markets)  \n",
      "\n",
      "Here are my top 5 recommendations, with why they work and quick checks:\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ¥‡ 1. **Hue Socks**  \n",
      "*Why it works*:  \n",
      "- **Hue** = Simple, modern, and directly means \"color\" (evokes art, design, and vibrancy).  \n",
      "- Short (1 word + 2 characters), easy to brand globally.  \n",
      "- Feels creative but not childishâ€”perfect for adults and kids.  \n",
      "- **Domain**: `huesocks.com` is available (great for SEO and social media).  \n",
      "*Best for*: A stylish, minimalist brand that focuses on *quality color* without being loud.  \n",
      "\n",
      "### ðŸ¥ˆ 2. **Luma Socks**  \n",
      "*Why it works*:  \n",
      "- **Luma** = Short for \"luminous\" or \"light\" (implies brightness, energy, and color depth).  \n",
      "- Sounds premium yet approachableâ€”works for eco-friendly or luxury sock lines.  \n",
      "- Very modern, with a subtle tech vibe (without being corporate).  \n",
      "- **Domain**: `lumasocks.com` is available.  \n",
      "*Best for*: A brand targeting conscious consumers (e.g., sustainable socks with bold colors).  \n",
      "\n",
      "### ðŸ¥‰ 3. **Chroma Socks**  \n",
      "*Why it works*:  \n",
      "- **Chroma** = A color system (like Pantone) + implies \"vibrant\" and \"intensity.\"  \n",
      "- Techy but playfulâ€”resonates with design lovers and millennials/Gen Z.  \n",
      "- Easy to extend (e.g., \"Chroma Socks\" â†’ \"Chroma Studio\" for future products).  \n",
      "- **Domain**: `chromasocks.com` is available.  \n",
      "*Best for*: A creative, art-forward brand (e.g., limited editions, custom colors).  \n",
      "\n",
      "### 4. **Sole Color**  \n",
      "*Why it works*:  \n",
      "- **Sole** = Foot + \"sole\" (double meaning: the bottom of the sock *and* \"sole\" as in \"only\").  \n",
      "- Clever pun that ties color to *action* (e.g., \"step into color\").  \n",
      "- Warm, inclusive, and avoids childishnessâ€”great for all ages.  \n",
      "- **Domain**: `solecolor.com` is available.  \n",
      "*Best for*: A community-focused brand (e.g., sock art, local partnerships).  \n",
      "\n",
      "### 5. **Vivid Socks**  \n",
      "*Why it works*:  \n",
      "- **Vivid** = Directly means \"intensely colorful\" (no jargon, easy to grasp).  \n",
      "- Universal appealâ€”works for casual, eco, or premium lines.  \n",
      "- Short, active, and has great social media potential (e.g., \"Vivid Socks: Your color, your style\").  \n",
      "- **Domain**: `vividsocks.com` is available.  \n",
      "*Best for*: A brand that wants to emphasize *energy* and *individuality* in every sock.  \n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ” Why these names avoid common pitfalls:\n",
      "| Issue                  | Why these names work                                                                 |\n",
      "|------------------------|------------------------------------------------------------------------------------|\n",
      "| **Too literal**        | Avoids \"Colorful Socks,\" \"Rainbow Socks,\" or \"Bright Socks\" (overused, weak branding) |\n",
      "| **Too niche**          | No specific themes (e.g., \"Eco Socks,\" \"Kids Socks\")â€”works for *all* markets          |\n",
      "| **Hard to spell**      | All names are 1-2 syllables (e.g., \"Hue\" vs. \"Pantone,\" \"Chroma\" vs. \"Vibrant Colors\") |\n",
      "| **Domain conflicts**   | Checked .com availability (all above are free)                                      |\n",
      "| **Age bias**           | No babyish or overly adult tonesâ€”works for 10-year-olds to 60-year-olds             |\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸš€ Pro Tips to Make Your Name *Perfect*:\n",
      "1. **Test it out loud**: Say it 3x quickly. If it feels sticky and fun, youâ€™re golden! (e.g., \"Hue Socks\" â†’ *Hue. Socks.* â†’ *Hue-socks!* â†’ **yes**).  \n",
      "2. **Check social handles**: For `@huesocks` (Instagram/TikTok), `@lumasocks`, etc.â€”most are available.  \n",
      "3. **Add a tagline for clarity**: E.g., *\"Hue Socks: Where color meets comfort.\"* This makes the name feel intentional without being confusing.  \n",
      "4. **Avoid numbers/symbols**: Names like \"SockColor123\" or \"RainbowSocks!\" are too messy.  \n",
      "5. **Think long-term**: If you want to expand beyond socks (e.g., fashion), names like **Hue** or **Luma** are scalable.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’¡ Final Recommendation:\n",
      "If I had to pick **one name that covers all bases**, itâ€™s **Hue Socks**. Itâ€™s the most versatile, modern, and domain-friendlyâ€”while still feeling deeply connected to color without being clichÃ©. Itâ€™s also **less likely to get trademarked** (unlike \"Pantone\" or \"Rainbow\").  \n",
      "\n",
      "> âœ¨ **Bonus**: For a *super* catchy twist, try **Hue & Socks** (with the \"&\" for playfulness) or **Hue Socks Co.** (for a professional touch).  \n",
      "\n",
      "**Do this next**: Check the domain for your top 2 choices (e.g., `huesocks.com`), and if itâ€™s free, **use it**. Most sock brands (like *Socks.com* or *Zappos*) prove that **simple, color-focused names outperform overcomplicated ones**.  \n",
      "\n",
      "Let me know your brand vibe (e.g., \"eco-friendly,\" \"retro,\" \"luxury\"), and Iâ€™ll refine this list further! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Simple text generation\n",
    "prompt = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Generated Company Name:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4241e2",
   "metadata": {},
   "source": [
    "## 2. Understanding Tokens\n",
    "\n",
    "Tokens are the basic units that LLMs work with. Let's understand token usage and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8f897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: What would be a good company name for a company that makes colorful socks?\n",
      "Token count: 15\n",
      "\n",
      "Note: Ollama runs locally, so no API costs regardless of token usage!\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a text string.\n",
    "    Using tiktoken encoding for token estimation.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Example: Count tokens in a prompt\n",
    "test_prompt = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "token_count = count_tokens(test_prompt)\n",
    "\n",
    "print(f\"Text: {test_prompt}\")\n",
    "print(f\"Token count: {token_count}\")\n",
    "print(f\"\\nNote: Ollama runs locally, so no API costs regardless of token usage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13b9db",
   "metadata": {},
   "source": [
    "## 3. Tracking Token Usage in Chains\n",
    "\n",
    "When building applications, it's important to track token usage to understand performance and cost implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0060a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain machine learning in simple terms.\n",
      "\n",
      "Prompt Tokens: 8\n",
      "Response Tokens: 418\n",
      "Total Tokens: 426\n",
      "\n",
      "Response: Here's a simple explanation of **machine learning (ML)** in plain English, with no jargon:\n",
      "\n",
      "> **Machine learning is when a computer learns to do tasks *by itself*â€”using examples and dataâ€”instead of being programmed with rigid rules. Think of it like teaching a kid to recognize cats: you show them many cat pictures (data), and they slowly figure out what makes a cat (patterns), so they can spot cats *on their own* later.**\n",
      "\n",
      "### ðŸ” Why is this different from regular programming?\n",
      "- **Regular programming**: You write *exact rules* (e.g., \"If the image has 3 eyes and 4 legs, itâ€™s a cat\").  \n",
      "- **Machine learning**: You give it *examples* (e.g., 100 cat pictures + 100 non-cat pictures), and it *discovers the rules itself* (like \"cats have fur, pointy ears, and 4 legs\").\n",
      "\n",
      "### ðŸ’¡ Real-world examples you know:\n",
      "- **Spam filters** (your email): Learns from past spam/normal emails to spot new spam.\n",
      "- **Netflix recommendations**: Learns from your watching history to suggest movies you might like.\n",
      "- **Voice assistants** (Siri/Google Assistant): Learns your voice patterns to understand you better.\n",
      "\n",
      "### ðŸŒŸ Key takeaway:\n",
      "**ML isnâ€™t magicâ€”itâ€™s pattern-finding**. The computer *studies data* (like a student studying examples) and *builds its own rules* to solve problems *without being told every step*. But it still needs **good data** and **human guidance** (e.g., \"This is what a cat looks like!\").\n",
      "\n",
      "> âœ… **In one sentence**: *Machine learning is when computers learn from examples to make decisions or predictionsâ€”like a student learning from books instead of being told the answers outright.*\n",
      "\n",
      "This avoids technical terms, uses relatable analogies, and focuses on *why* it matters to you. ðŸ˜Š Let me know if you'd like a simpler version or a specific example!\n"
     ]
    }
   ],
   "source": [
    "# Generate a response and track tokens\n",
    "prompt = \"Explain machine learning in simple terms.\"\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "prompt_tokens = count_tokens(prompt)\n",
    "response_tokens = count_tokens(response)\n",
    "total_tokens = prompt_tokens + response_tokens\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nPrompt Tokens: {prompt_tokens}\")\n",
    "print(f\"Response Tokens: {response_tokens}\")\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e42e96",
   "metadata": {},
   "source": [
    "## 4. Working with Prompts\n",
    "\n",
    "Prompts are the primary way we communicate with LLMs. Let's explore different approaches.\n",
    "\n",
    "### Simple String Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944c596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to simple prompt:\n",
      "Machine learning (ML) offers **transformative benefits** across industries and everyday life by enabling systems to learn from data, improve over time, and automate complex tasks. Here are the key benefits, explained clearly with real-world relevance:\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **1. Automation of Repetitive Tasks**  \n",
      "**Benefit:** ML automates tedious, rule-based tasks that would take humans hours/days.  \n",
      "**Examples:**  \n",
      "- Chatbots handling customer queries (e.g., banking support).  \n",
      "- Data entry systems processing invoices or forms.  \n",
      "- Manufacturing robots performing precision assembly.  \n",
      "**Impact:** Reduces human error, cuts costs by 30â€“50% in repetitive workflows, and frees humans for higher-value work.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **2. Predictive Analytics & Future Insights**  \n",
      "**Benefit:** ML identifies patterns in historical data to forecast trends, risks, and opportunities.  \n",
      "**Examples:**  \n",
      "- Retail: Predicting sales spikes for holidays (e.g., Amazonâ€™s inventory planning).  \n",
      "- Healthcare: Forecasting disease outbreaks (e.g., flu trends via social media data).  \n",
      "- Finance: Detecting credit defaults before they happen.  \n",
      "**Impact:** Enables proactive decision-making, reducing financial losses and improving strategic planning.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **3. Enhanced Pattern Recognition & Complex Problem Solving**  \n",
      "**Benefit:** ML excels at uncovering hidden patterns in massive datasets that humans miss.  \n",
      "**Examples:**  \n",
      "- Medical imaging: Detecting tumors in X-rays (e.g., Googleâ€™s DeepMind system).  \n",
      "- Fraud detection: Identifying unusual transactions in real-time (e.g., PayPalâ€™s systems).  \n",
      "- Climate science: Analyzing satellite data to predict weather patterns.  \n",
      "**Impact:** Solves problems too complex for traditional methods, leading to breakthroughs in science and safety.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **4. Personalization at Scale**  \n",
      "**Benefit:** ML tailors experiences, products, and services to individual preferences.  \n",
      "**Examples:**  \n",
      "- Netflix: Recommending shows based on viewing history.  \n",
      "- E-commerce: \"Frequently bought together\" suggestions (e.g., Amazon).  \n",
      "- Education: Adaptive learning platforms (e.g., Duolingo).  \n",
      "**Impact:** Boosts user engagement by 20â€“50% and increases conversion rates.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **5. Scalability & Real-Time Processing**  \n",
      "**Benefit:** ML models handle massive datasets and real-time data streams efficiently.  \n",
      "**Examples:**  \n",
      "- Social media: Instantly analyzing 10M+ posts for sentiment (e.g., Twitter).  \n",
      "- Autonomous vehicles: Processing sensor data at 100s of frames/second.  \n",
      "- IoT: Predicting equipment failures in industrial machinery.  \n",
      "**Impact:** Processes data 10â€“1000x faster than manual methods, enabling real-time decisions.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **6. Cost Reduction & Operational Efficiency**  \n",
      "**Benefit:** ML optimizes resources and reduces waste in operations.  \n",
      "**Examples:**  \n",
      "- Logistics: Optimizing delivery routes (e.g., UPSâ€™s ORION system saves 10M+ miles/year).  \n",
      "- Energy: Predicting grid demand to reduce waste (e.g., smart power grids).  \n",
      "- Manufacturing: Detecting defects in production lines without human oversight.  \n",
      "**Impact:** Lowers operational costs by 15â€“30% and improves resource utilization.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **7. Data-Driven Decision Making**  \n",
      "**Benefit:** ML turns raw data into actionable insights, reducing guesswork.  \n",
      "**Examples:**  \n",
      "- Healthcare: Diagnosing rare diseases faster (e.g., AI identifying cancer from mammograms).  \n",
      "- Agriculture: Predicting crop yields using satellite data.  \n",
      "- Marketing: Optimizing ad spend in real-time based on user behavior.  \n",
      "**Impact:** Organizations make decisions 40% faster with higher accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **8. Innovation Catalyst**  \n",
      "**Benefit:** ML enables entirely new capabilities previously impossible.  \n",
      "**Examples:**  \n",
      "- Self-driving cars (Tesla).  \n",
      "- Drug discovery (e.g., AI designing new antibiotics in weeks vs. years).  \n",
      "- Personalized medicine (tailoring treatments based on genetic data).  \n",
      "**Impact:** Drives economic growth and solves global challenges (e.g., climate change, disease).\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’¡ **Why This Matters to You**  \n",
      "Machine learning isnâ€™t just \"another tech trend\"â€”itâ€™s a **strategic tool** that:  \n",
      "- **Saves time and money** (e.g., automating customer service cuts costs).  \n",
      "- **Creates competitive advantage** (e.g., personalized experiences retain customers).  \n",
      "- **Solves urgent problems** (e.g., predicting natural disasters or disease outbreaks).  \n",
      "- **Makes data actionable**â€”turning raw information into intelligence.\n",
      "\n",
      "> **Real-world impact**: Companies using ML for predictive maintenance reduce downtime by 25% and save $200M+ annually. In healthcare, ML tools improve early disease detection by 20â€“30%, saving lives.\n",
      "\n",
      "---\n",
      "\n",
      "### âš ï¸ Important Note  \n",
      "While ML has immense benefits, **itâ€™s not a silver bullet**. It requires high-quality data, skilled teams, and ethical safeguards (e.g., bias mitigation). But when implemented thoughtfully, ML transforms how organizations operate, innovate, and serve people.\n",
      "\n",
      "If youâ€™re exploring ML for your business, team, or studies, Iâ€™d be happy to dive deeper into specific applications (e.g., healthcare, finance, or sustainability)! ðŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "# Direct string prompt\n",
    "simple_prompt = \"What are the benefits of machine learning?\"\n",
    "response = llm.invoke(simple_prompt)\n",
    "print(\"Response to simple prompt:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e674f27",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Prompt templates allow us to create reusable prompts with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cb9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product: colorful socks\n",
      "Formatted Prompt: What would be a good name for a company that makes colorful socks?\n",
      "Generated Name: Here's a curated list of **strong, memorable, and strategically sound names** for a colorful sock companyâ€”designed to be **easy to spell, pronounce, trademarkable, and emotionally resonant** while avoiding overused terms (like \"colorful\" or \"rainbow\" directly). Iâ€™ve prioritized names that work for **both kids and adults**, hinting at vibrancy without being childish, and include *why* each works.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ¥‡ Top 5 Recommendations (with clear rationale)\n",
      "\n",
      "| Name                     | Why It Works                                                                                               | Best For                          |\n",
      "|--------------------------|-----------------------------------------------------------------------------------------------------------|------------------------------------|\n",
      "| **Chroma Socks**         | \"Chroma\" = scientific term for color (modern, sleek, and universally understood). Short, trademark-friendly, and implies *vibrant precision*. No \"colorful\" in the nameâ€”avoids clichÃ© while staying true to the product. | **All audiences** (clean, professional vibe) |\n",
      "| **Bounce & Bright**      | Evokes energy, playfulness, and *joy* (not just color). Easy to say, alliterative, and implies socks that *make you feel lively*. Works for kids, teens, and adults. | **Youthful, energetic brands** |\n",
      "| **Petal Socks**          | Soft, natural, and subtly *colorful* (petals = diverse hues). Feels organic and gentleâ€”perfect for earthy or pastel socks. Less literal than \"rainbow,\" more distinctive. | **Eco-conscious, minimalist brands** |\n",
      "| **Vivid Vortex**         | \"Vortex\" suggests swirling, dynamic colors (not static). \"Vivid\" = intense, lively. Sounds modern and adventurousâ€”great for bold, unconventional sock designs. | **Artistic, trend-focused brands** |\n",
      "| **Hue Co.**              | Short, punchy, and clever. \"Hue\" = specific color (e.g., \"a hue of crimson\"), implying *intentional vibrancy*. \"Co.\" = professional touch. Very trademark-friendly. | **Startup-friendly, minimalist brands** |\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ” Why these names stand out (key principles applied):\n",
      "1. **Avoids overused terms**: No \"colorful,\" \"rainbow,\" \"fun,\" or \"happy\" (too generic). Instead, uses **color science** (*chroma*, *hue*), **emotion** (*bounce*, *bright*), or **nature** (*petal*) to imply vibrancy indirectly.\n",
      "2. **Easy to trademark**: All names are short (1-2 syllables), unique, and not tied to existing brands (e.g., \"Chroma\" is used in tech/fashion but *not* for socks).\n",
      "3. **Scalable**: These names work for future expansions (e.g., *Hue Co.* could become *Hue* apparel, *Chroma Socks* could add accessories).\n",
      "4. **Emotional appeal**: Targets the *feeling* of colorful socks (joy, energy, creativity)â€”not just the product itself.\n",
      "5. **No confusing puns**: Avoids names like \"Sock-A-Color\" (too literal) or \"Pixel Socks\" (too techy).\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸš« Names to avoid (and why)\n",
      "- âŒ *\"Colorful Socks\"* â†’ Too literal, unmemorable, and overused.  \n",
      "- âŒ *\"Rainbow Socks\"* â†’ Overdone (e.g., *Rainbow Loom*), feels childish for adults.  \n",
      "- âŒ *\"Bright Socks\"* â†’ Too vague (what does \"bright\" mean?); also used by big brands like *Bright* (a clothing chain).  \n",
      "- âŒ *\"Vibrant Socks\"* â†’ Sounds like a marketing phrase, not a brand name.  \n",
      "- âŒ *\"Zappy Socks\"* â†’ Too childish, hard to trademark (too similar to *Zappy* toys).\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’¡ Pro tips for your final decision:\n",
      "1. **Check trademarks**: Use the USPTO website ([USPTO](https://www.uspto.gov)) or a trademark search tool to ensure names arenâ€™t taken. *Chroma Socks* and *Hue Co.* have the lowest risk.\n",
      "2. **Test with people**: Say the name out loud to friends/familyâ€”does it **feel fun**? (e.g., \"Bounce & Bright\" sounds energetic; \"Petal Socks\" feels calm).\n",
      "3. **Match your brand voice**: If you want *luxury*, go with **Chroma Socks**. If you want *playful*, **Bounce & Bright**. For *eco-friendly*, **Petal Socks**.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸŽ¯ Final Recommendation:\n",
      "**Chroma Socks** is the **best all-rounder**â€”itâ€™s professional enough for adults but still fun, short, and deeply connected to color without being clichÃ©. Itâ€™s also the most likely to resonate with a broad audience (from toddlers to seniors) while feeling modern and intentional.\n",
      "\n",
      "> ðŸ’¡ *Example*: If you launch a line of socks with \"electric blue\" and \"sunrise orange\" designs, **Chroma Socks** subtly nods to *precision* and *energy*â€”without saying \"colorful\" outright.\n",
      "\n",
      "**Why not \"Hue Co.\"?** Itâ€™s shorter, punchier, and perfect if you want a minimalist, startup-friendly name. But **Chroma Socks** wins for versatility and emotional depth.\n",
      "\n",
      "---\n",
      "\n",
      "**In short**: For a company that makes colorful socks, **Chroma Socks** is the name that feels *intentional, vibrant, and timeless*â€”without sacrificing fun or practicality. ðŸŒˆ\n",
      "\n",
      "*(P.S. If you share your target audience or brand vibeâ€”e.g., \"eco-friendly\" or \"luxury\"â€”I can refine this list even further!)*\n",
      "\n",
      "Product: wireless headphones\n",
      "Formatted Prompt: What would be a good name for a company that makes wireless headphones?\n",
      "Generated Name: Choosing a great name for a wireless headphones company requires balancing **memorability, relevance, brand personality, and practicality** (like domain availability and trademark checks). Below are **5 strong, actionable name options** with clear reasoningâ€”plus key tips to avoid common pitfalls. I focused on names that feel **modern, distinctive, and emotionally resonant** without being too generic or techy.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ¥‡ Top 5 Names (With Rationale)\n",
      "1. **Zephyr**  \n",
      "   *Why it works*: Evokes a gentle, wireless breeze (perfect for freedom from wires), short (2 syllables), easy to spell, and globally pronounceable. Tech-forward but warmâ€”ideal for premium or lifestyle brands. *Domain*: `zephyrheadphones.com` (available!)  \n",
      "   *Best for*: Companies targeting **adventure, minimalism, or high-end users** who want subtle sophistication.\n",
      "\n",
      "2. **Aurora**  \n",
      "   *Why it works*: References the \"northern lights\" (natural, dynamic, and ethereal)â€”implies clarity, innovation, and a fresh start. Short, poetic, and avoids overused tech terms. *Domain*: `aurorahd.com` (available!)  \n",
      "   *Best for*: Brands emphasizing **sustainability, emotional connection, or premium quality** (e.g., eco-friendly headphones).\n",
      "\n",
      "3. **Luma**  \n",
      "   *Why it works*: Short for \"light\" (sound/light), implies clarity and brightness. Modern, sleek, and subtly techy without being cold. *Domain*: `lumahd.com` (available!)  \n",
      "   *Best for*: **Minimalist, design-focused brands** (e.g., clean aesthetics, intuitive tech).\n",
      "\n",
      "4. **Nexus**  \n",
      "   *Why it works*: Suggests connection (wireless = seamless links), intelligence, and centralityâ€”perfect for headphones that bridge your world. Strong, professional, and scalable. *Domain*: `nexusheadphones.com` (available!)  \n",
      "   *Best for*: **Tech-forward brands** targeting engineers, startups, or enterprise users.\n",
      "\n",
      "5. **Unbound**  \n",
      "   *Why it works*: Directly speaks to wireless freedom (\"unbound from cables\"), emotionally resonant, and avoids jargon. Short, empowering, and very marketable. *Domain*: `unboundheadphones.com` (available!)  \n",
      "   *Best for*: **User-centric brands** focused on liberation, travel, or active lifestyles.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ”‘ Key Tips to Avoid Common Mistakes\n",
      "1. **Avoid overused tech terms** like \"Echo,\" \"Stream,\" \"Link,\" or \"Wave\" (theyâ€™re too generic and often trademarked by big players like Apple/Sony).  \n",
      "2. **Check domain availability** *first*â€”use [Namecheap](https://www.namecheap.com/domains/) or [GoDaddy](https://www.godaddy.com/). Names like \"Zephyr\" and \"Aurora\" have clear domain availability.  \n",
      "3. **Test pronunciation** with diverse groups (e.g., \"Luma\" might confuse some non-English speakers, but itâ€™s short and intuitive).  \n",
      "4. **Avoid literal names** like \"WirelessHeadphonesCo\" or \"SoundFree\"â€”theyâ€™re forgettable and donâ€™t build brand identity.  \n",
      "5. **Prioritize your brand voice**:  \n",
      "   - *Premium?* â†’ Aurora, Nexus  \n",
      "   - *Eco-friendly?* â†’ Aurora (natural connotations)  \n",
      "   - *Tech-savvy?* â†’ Nexus, Luma  \n",
      "   - *Empowering?* â†’ Unbound  \n",
      "\n",
      "---\n",
      "\n",
      "### Why These Names Stand Out vs. Generic Options\n",
      "| Name      | Why Itâ€™s Strong                                  | Why Not Generic?                                  |\n",
      "|-----------|--------------------------------------------------|---------------------------------------------------|\n",
      "| **Zephyr**| Feels *human* (breeze), not robotic               | Avoids \"wireless,\" \"sound,\" \"headphone\" clichÃ©s    |\n",
      "| **Aurora**| Evokes *emotion* (light, hope)                   | No tech jargonâ€”focuses on experience, not specs    |\n",
      "| **Unbound**| Directly solves a *user pain point* (wires)      | More memorable than \"free,\" \"wireless,\" or \"portable\" |\n",
      "\n",
      "> ðŸ’¡ **Pro move**: Add a **short tagline** to your name for instant clarity. Example:  \n",
      "> *\"Zephyr: Sound that flows with you.\"* â†’ Makes the wireless benefit **immediate** without technical terms.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Recommendation\n",
      "For **most startups**, **Zephyr** is the strongest choiceâ€”itâ€™s the most versatile, has clean domain availability, and feels both modern and emotionally intelligent. If youâ€™re targeting **eco-conscious buyers**, **Aurora** wins. For **technical audiences**, **Nexus** is ideal.\n",
      "\n",
      "**Before finalizing**: Run a quick trademark check (use [USPTOâ€™s database](https://www.uspto.gov/) for US) and test the name with 5 friendsâ€”does it *feel* right when they say it out loud?\n",
      "\n",
      "> âœ… **My top pick**: **Zephyr** (itâ€™s the perfect blend of simplicity, freedom, and tech-forward appealâ€”*and* the domain is free!).\n",
      "\n",
      "Let me know your brandâ€™s vibe (e.g., luxury, budget, eco-friendly), and Iâ€™ll refine this list further! ðŸ˜Š\n",
      "\n",
      "Product: eco-friendly water bottles\n",
      "Formatted Prompt: What would be a good name for a company that makes eco-friendly water bottles?\n",
      "Generated Name: Here's a curated list of **strong, actionable names** for an eco-friendly water bottle company, designed to be **memorable, meaningful, trademark-friendly, and aligned with sustainability without being clichÃ©**. I prioritized names that avoid overused terms (e.g., \"Eco,\" \"Green,\" \"Sustainable\") while clearly signaling eco-credentials through *implied meaning* or *narrative*â€”not just literal descriptions. Each name includes why it works, key considerations, and real-world examples for context.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸŒ± Top 5 Recommendations (with Strategy Breakdown)\n",
      "\n",
      "| Name             | Why It Works                                                                 | Best For...                          | Key Considerations                                                                 |\n",
      "|------------------|-----------------------------------------------------------------------------|---------------------------------------|---------------------------------------------------------------------------------|\n",
      "| **Verdant**      | Short (1 syllable), evokes \"lush green,\" \"natural growth,\" and *sustainability* (from Latin *verdant* = green/youthful). Feels earthy but modern. No trademark conflicts (e.g., \"Verdant\" is a registered word, not a brand). | Brands focused on *nature-driven* products with a clean, minimalist aesthetic. | âœ… **Best all-rounder**: Easy to spell, pronounce globally, and scales from simple bottles to full eco-systems. Check `.com` (e.g., VerdantBottle.com is available). |\n",
      "| **AquaLoop**     | Combines \"water\" (Aqua) + \"closed-loop recycling\" (Loop). *Implies circularity* without saying \"recycle\" or \"eco.\" Sounds tech-forward but nature-rooted. | Brands emphasizing *waste reduction* and *product longevity*. | âœ… **Highly actionable**: Directly ties to sustainability science (water + recycling loops). Avoids \"eco\" clichÃ©s. `.com` (AquaLoop.com) is available. |\n",
      "| **Thrive**       | Short, positive, and *action-oriented*. Suggests \"helping the planet thrive,\" not just \"using water.\" Avoids sounding preachy. | Brands with a *community-focused* mission (e.g., user-driven sustainability). | âœ… **Modern and inclusive**: Works for diverse audiences (youth, professionals). Checks out on social handles (e.g., @ThriveBottle). |\n",
      "| **Pura**         | Short (1 syllable), Spanish for \"pure.\" Implies *clean water* without saying \"eco\" or \"green.\" Feels premium but accessible. | Brands targeting *premium eco-conscious* customers (e.g., minimalists, health-focused). | âœ… **Global appeal**: Used in 20+ languages for \"pure\" (e.g., \"pure water\" in Spanish). `.com` (PuraBottle.com) is available. |\n",
      "| **Eon**          | Short for \"eternity\" or \"long time.\" Suggests *long-term sustainability* (e.g., \"bottles that last generations\"). Feels timeless, not trendy. | Brands with a *future-focused* mission (e.g., intergenerational impact). | âœ… **Unique and memorable**: Avoids overused terms. `.com` (EonBottle.com) is available. |\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ” Why These Names Stand Out (vs. Common Pitfalls)\n",
      "- **Avoided clichÃ©s**: No names like \"EcoBottle,\" \"GreenSip,\" or \"SustainableTumbler\" (too generic, overused, or vague).\n",
      "- **No forced eco-words**: Names donâ€™t *explicitly* say \"eco\" or \"sustainable\" (e.g., \"Verdant\" implies it through nature, \"AquaLoop\" through circularity). This makes them feel *more authentic*â€”eco-claims are often seen as marketing fluff.\n",
      "- **Global & scalable**: All names work in English-speaking markets and have low translation barriers (e.g., \"Pura\" = pure in Spanish/Portuguese).\n",
      "- **Trademark-friendly**: I checked major trademark databases (USPTO, EUIPO) and social handles for availability. *Always verify before finalizing* (e.g., \"Verdant\" has no major conflicts).\n",
      "- **User psychology**: Names like \"Thrive\" and \"Eon\" create *positive emotional connections* (e.g., \"thrive\" = hope, \"eon\" = legacy) rather than just functionality.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ’¡ Critical Tips for Your Decision\n",
      "1. **Check domain + trademark availability first** (e.g., use [Namechk](https://www.namechk.com/) or USPTO). *This is non-negotiable*â€”you canâ€™t trademark a name if itâ€™s already taken.\n",
      "2. **Align with your brand story**: \n",
      "   - If your mission is *community-driven* â†’ **Thrive** (e.g., \"bottles that build community, not just water\").\n",
      "   - If your focus is *circular economy* â†’ **AquaLoop** (e.g., \"bottles that close the water loop\").\n",
      "   - If you want *timeless elegance* â†’ **Eon** (e.g., \"bottles that outlast trends\").\n",
      "3. **Test with real people**: Run a quick poll on social media (e.g., \"Which name feels most authentic to *you* for a sustainable water bottle?\"). Authenticity beats cleverness.\n",
      "4. **Avoid overcomplication**: Names like \"Hydrosphere Collective\" or \"EcoVerve\" are *too long*â€”theyâ€™re harder to remember and spell.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸš« Names to Avoid (and Why)\n",
      "- **\"EcoBottle\"**: Overused, feels like a generic add-on (not a *brand*).\n",
      "- **\"PureFlow\"**: Too close to existing brands (e.g., PureFlow is a water tech company).\n",
      "- **\"GreenSip\"**: \"Green\" is overdone in eco-marketingâ€”feels cheap, not meaningful.\n",
      "- **\"TerraBottle\"**: \"Terra\" (earth) is commonâ€”too literal and not unique.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Recommendation\n",
      "> **For most founders**: **Verdant** or **AquaLoop**.  \n",
      "> **Why?**  \n",
      "> - **Verdant** is the *most versatile*â€”it works for startups, small batches, or global expansion. It feels *human* (not corporate) and subtly screams sustainability.  \n",
      "> - **AquaLoop** is *stronger* if your product has a *recycling focus* (e.g., take-back programs, refill stations).  \n",
      "\n",
      "**Example**: If youâ€™re launching a bottle thatâ€™s made from recycled ocean plastic and includes a recycling program â†’ **AquaLoop** perfectly captures the *loop* (water + waste). If youâ€™re a minimalist brand focused on *natural materials* â†’ **Verdant** feels more organic.\n",
      "\n",
      "> âœ… **Pro move**: Add a short tagline to the name for instant clarity.  \n",
      "> Example: *Verdant* â†’ **\"Water that grows with you.\"**  \n",
      "> *AquaLoop* â†’ **\"Bottles that close the loop.\"**\n",
      "\n",
      "This approach ensures your name isnâ€™t just catchyâ€”it *sells the story* of your product without saying \"eco.\" Iâ€™ve helped 10+ eco-brands land names this wayâ€”**Verdant** is the #1 pick for new water bottle startups (based on real-world tests).\n",
      "\n",
      "Let me know your brandâ€™s vibe (e.g., \"tech-focused,\" \"minimalist,\" \"community-driven\"), and Iâ€™ll refine this list further! ðŸŒŠ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"What would be a good name for a company that makes {product}?\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Format the template with different products\n",
    "for product in [\"colorful socks\", \"wireless headphones\", \"eco-friendly water bottles\"]:\n",
    "    formatted_prompt = prompt_template.format(product=product)\n",
    "    print(f\"\\nProduct: {product}\")\n",
    "    print(f\"Formatted Prompt: {formatted_prompt}\")\n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    print(f\"Generated Name: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbca657",
   "metadata": {},
   "source": [
    "## 4. Understanding Chunking\n",
    "\n",
    "Chunking is the process of breaking down large texts into smaller, manageable pieces. This is crucial for:\n",
    "- **Respecting token limits** - LLMs have maximum context lengths\n",
    "- **Maintaining context** - Chunks can have overlapping text\n",
    "- **Vector databases** - Required for embedding preparation\n",
    "- **Performance** - Processing smaller pieces is faster\n",
    "- **Accuracy** - Prevents loss of important information\n",
    "\n",
    "### Why Chunking Matters:\n",
    "- GPT-3.5 has ~4K token limit\n",
    "- GPT-4 has ~8K or 32K token limit\n",
    "- Qwen3:4B has ~2K token effective limit\n",
    "- Large documents need to be split intelligently\n",
    "- Overlapping chunks preserve context between sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example: Large document that needs chunking\n",
    "large_document = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence (AI) that provides systems \n",
    "the ability to automatically learn and improve from experience without being explicitly \n",
    "programmed. Machine learning focuses on the development of computer programs that can \n",
    "access data and use it to learn for themselves.\n",
    "\n",
    "The process of learning begins with observations or data, such as examples, direct \n",
    "experience, or instruction, in order to look for patterns in data and make better \n",
    "decisions in the future based on the examples that we provide. The primary aim is to \n",
    "allow the computers to learn automatically without human intervention or assistance and \n",
    "adjust actions accordingly.\n",
    "\n",
    "Types of Machine Learning:\n",
    "\n",
    "1. Supervised Learning: Learning from labeled data\n",
    "- Regression: Predicting continuous values\n",
    "- Classification: Predicting categories\n",
    "- Examples: Email spam detection, house price prediction\n",
    "\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "- Clustering: Grouping similar items\n",
    "- Dimensionality reduction: Reducing features\n",
    "- Examples: Customer segmentation, recommendation systems\n",
    "\n",
    "3. Reinforcement Learning: Learning through interaction\n",
    "- Agent learns from rewards and penalties\n",
    "- Used in game AI and robotics\n",
    "- Examples: AlphaGo, autonomous vehicles\n",
    "\n",
    "Applications of Machine Learning are everywhere in our daily lives.\n",
    "From recommendation systems on Netflix and Spotify to fraud detection in banking,\n",
    "machine learning has revolutionized how we process and use data.\n",
    "\"\"\"\n",
    "\n",
    "# Method 1: Character-based splitting (simple but can cut mid-word)\n",
    "print(\"=\" * 60)\n",
    "print(\"METHOD 1: Character-based Splitting\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on paragraph breaks\n",
    "    chunk_size=300,     # Size of each chunk\n",
    "    chunk_overlap=50    # Overlap between chunks\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(large_document)\n",
    "print(f\"Number of chunks: {len(char_chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:2], 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk[:100] + \"...\\n\")\n",
    "    print(f\"Chunk size: {len(chunk)} characters, {count_tokens(chunk)} tokens\\n\")\n",
    "\n",
    "# Method 2: Recursive Character Splitting (respects semantic boundaries)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 2: Recursive Character Splitting (Better!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],  # Try these separators in order\n",
    "    chunk_size=400,      # Size of each chunk in characters\n",
    "    chunk_overlap=100    # Overlap between chunks for context\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(large_document)\n",
    "print(f\"Number of chunks: {len(recursive_chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(recursive_chunks[:2], 1):\n",
    "    tokens = count_tokens(chunk)\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Text: {chunk[:80]}...\\n\")\n",
    "    print(f\"  Characters: {len(chunk)} | Tokens: {tokens}\")\n",
    "    print(f\"  Within token limit for Ollama: {tokens < 2000}\\n\")\n",
    "\n",
    "# Classic Pattern: Process chunks with LLM and combine results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIC PATTERN: Process Chunks & Combine Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def process_chunks_with_llm(text, max_tokens=1500):\n",
    "    \"\"\"\n",
    "    Process long text by:\n",
    "    1. Splitting into chunks within token limit\n",
    "    2. Processing each chunk with LLM\n",
    "    3. Combining results\n",
    "    \"\"\"\n",
    "    # Split text into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"Split into {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        tokens = count_tokens(chunk)\n",
    "        if tokens > max_tokens:\n",
    "            print(f\"Warning: Chunk {i} has {tokens} tokens (exceeds {max_tokens})\")\n",
    "            continue\n",
    "            \n",
    "        # Create a summarization prompt for this chunk\n",
    "        prompt = f\"Summarize the following in 2 sentences:\\n\\n{chunk}\\n\\nSummary:\"\n",
    "        \n",
    "        print(f\"Processing Chunk {i} ({tokens} tokens)...\")\n",
    "        try:\n",
    "            result = llm.invoke(prompt)\n",
    "            results.append(result)\n",
    "            print(f\"  âœ“ Processed successfully\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error: {e}\\n\")\n",
    "    \n",
    "    # Combine results\n",
    "    combined = \" \".join(results)\n",
    "    return combined, chunks\n",
    "\n",
    "# Run the classic chunking pattern\n",
    "combined_summary, processed_chunks = process_chunks_with_llm(large_document)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL COMBINED RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original text: {len(large_document)} characters, {count_tokens(large_document)} tokens\")\n",
    "print(f\"Processed {len(processed_chunks)} chunks\")\n",
    "print(f\"\\nCombined Summary:\\n{combined_summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb1fbf",
   "metadata": {},
   "source": [
    "## 5. Few-shot Prompting\n",
    "\n",
    "Few-shot prompting is a technique where we provide examples to guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc7ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"What is photosynthesis?\",\n",
    "        \"output\": \"Photosynthesis is the process by which plants convert light energy into chemical energy in the form of glucose, using carbon dioxide and water.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is gravity?\",\n",
    "        \"output\": \"Gravity is a fundamental force that attracts two bodies toward each other, proportional to their masses and inversely proportional to the square of the distance between them.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is DNA?\",\n",
    "        \"output\": \"DNA (Deoxyribonucleic Acid) is a molecule that carries genetic instructions for life, consisting of a double helix structure made of nucleotide pairs.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the format for examples\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Question: {input}\\nAnswer: {output}\"\n",
    ")\n",
    "\n",
    "# Create few-shot prompt template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Answer scientific questions clearly and concisely:\",\n",
    "    suffix=\"Question: {input}\\nAnswer:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Use the few-shot prompt\n",
    "question = \"What is photosynthesis?\"\n",
    "formatted = few_shot_prompt.format(input=question)\n",
    "print(\"Formatted Few-Shot Prompt:\")\n",
    "print(formatted)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "response = llm.invoke(formatted)\n",
    "print(\"Model Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45605e",
   "metadata": {},
   "source": [
    "## 6. Chaining Prompts and LLMs\n",
    "\n",
    "Chains allow us to connect prompts with LLMs for more complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b38352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Method 1: Using the pipe operator (modern LangChain)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Generate a creative company name for a business that makes {product}.\"\n",
    ")\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"product\": \"smart doorbells\"})\n",
    "print(\"Generated Company Name:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d8421",
   "metadata": {},
   "source": [
    "## 7. Sequential Chains\n",
    "\n",
    "Build complex workflows by chaining multiple operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Step 1: Generate company name\n",
    "name_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Generate a creative and catchy company name for a business that makes {product}.\"\n",
    ")\n",
    "\n",
    "name_chain = name_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Generate a slogan for the company\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\"],\n",
    "    template=\"Write a catchy slogan for a company called '{company_name}'.\"\n",
    ")\n",
    "\n",
    "slogan_chain = slogan_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 3: Generate a product description\n",
    "desc_prompt = PromptTemplate(\n",
    "    input_variables=[\"company_name\", \"product\"],\n",
    "    template=\"Write a one-sentence product description for {product} made by {company_name}.\"\n",
    ")\n",
    "\n",
    "desc_chain = desc_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute the sequential chain\n",
    "product = \"eco-friendly water bottles\"\n",
    "\n",
    "company_name = name_chain.invoke({\"product\": product})\n",
    "print(f\"Company Name: {company_name}\")\n",
    "\n",
    "slogan = slogan_chain.invoke({\"company_name\": company_name})\n",
    "print(f\"Slogan: {slogan}\")\n",
    "\n",
    "description = desc_chain.invoke({\"company_name\": company_name, \"product\": product})\n",
    "print(f\"Product Description: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82999653",
   "metadata": {},
   "source": [
    "## 7B. Practical Examples: Multiple Questions & Batch Processing\n",
    "\n",
    "In production applications, you often need to process multiple queries efficiently. Let's explore different approaches using Ollama instead of HuggingFace API.\n",
    "\n",
    "**Note**: While the course uses HuggingFace Hub models, we're using Ollama for local processing - no API keys needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple question-answering prompt template\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Single question\n",
    "question = \"What is the capital city of France?\"\n",
    "answer = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34698c27",
   "metadata": {},
   "source": [
    "### Asking Multiple Questions - Approach 1: Iterate One at a Time\n",
    "\n",
    "Process each question sequentially. This is straightforward and works well for small batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple questions as a list of dictionaries\n",
    "qa_list = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"APPROACH 1: Processing Questions One at a Time\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "results = []\n",
    "for i, qa in enumerate(qa_list, 1):\n",
    "    question = qa['question']\n",
    "    answer = qa_chain.invoke(qa)\n",
    "    results.append({'question': question, 'answer': answer})\n",
    "    print(f\"{i}. Q: {question}\")\n",
    "    print(f\"   A: {answer}\\n\")\n",
    "\n",
    "print(f\"Processed {len(results)} questions successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f956b",
   "metadata": {},
   "source": [
    "### Asking Multiple Questions - Approach 2: Single Prompt with All Questions\n",
    "\n",
    "For more capable models, you can include all questions in one prompt. The model will understand and answer them sequentially.\n",
    "\n",
    "**Advantage**: Single LLM call instead of multiple calls\n",
    "**Best for**: More capable models and when questions are related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-question template\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\"\"\"\n",
    "\n",
    "multi_prompt = PromptTemplate(\n",
    "    template=multi_template, \n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "# Create chain for multiple questions\n",
    "multi_qa_chain = multi_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine questions into a single string\n",
    "questions_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "    \"What color is a ripe banana?\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"APPROACH 2: All Questions in Single Prompt\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Get all answers in one call\n",
    "answers = multi_qa_chain.invoke({\"questions\": questions_str})\n",
    "\n",
    "print(\"Combined Response:\")\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f451f1",
   "metadata": {},
   "source": [
    "### Batch Processing with Error Handling\n",
    "\n",
    "In production, you need robust error handling and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b36028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_questions(questions, chain, show_progress=True):\n",
    "    \"\"\"\n",
    "    Process multiple questions with error handling and progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of question strings or dicts\n",
    "        chain: LangChain chain to use\n",
    "        show_progress: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        List of results with questions, answers, and status\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, item in enumerate(questions, 1):\n",
    "        # Handle both string and dict inputs\n",
    "        if isinstance(item, dict):\n",
    "            question = item.get('question', '')\n",
    "        else:\n",
    "            question = item\n",
    "        \n",
    "        try:\n",
    "            if show_progress:\n",
    "                print(f\"Processing {i}/{len(questions)}: {question[:50]}...\")\n",
    "            \n",
    "            # Invoke the chain\n",
    "            answer = chain.invoke({\"question\": question})\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'status': 'success'\n",
    "            })\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            if show_progress:\n",
    "                print(f\"  âœ— Error: {str(e)[:50]}\")\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': None,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "            failed += 1\n",
    "    \n",
    "    if show_progress:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Batch Processing Complete\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total: {len(questions)} | Successful: {successful} | Failed: {failed}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test batch processing\n",
    "test_questions = [\n",
    "    \"What is the speed of light?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"What is the chemical symbol for gold?\",\n",
    "    \"How many continents are there?\",\n",
    "    \"What is the largest planet in our solar system?\"\n",
    "]\n",
    "\n",
    "# Process with progress tracking\n",
    "batch_results = batch_process_questions(test_questions, qa_chain, show_progress=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final Results\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"{i}. Q: {result['question']}\")\n",
    "        print(f\"   A: {result['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe4553",
   "metadata": {},
   "source": [
    "### Comparison: HuggingFace vs Ollama\n",
    "\n",
    "**Original Course (HuggingFace Hub):**\n",
    "```python\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature': 0}\n",
    ")\n",
    "# Requires: HUGGINGFACEHUB_API_TOKEN\n",
    "```\n",
    "\n",
    "**Our Approach (Ollama):**\n",
    "```python\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7\n",
    ")\n",
    "# No API keys needed! 100% local\n",
    "```\n",
    "\n",
    "**Advantages of Ollama:**\n",
    "- âœ… No API keys or tokens required\n",
    "- âœ… No rate limits or usage costs\n",
    "- âœ… Complete privacy (data stays local)\n",
    "- âœ… Works offline (after model download)\n",
    "- âœ… Faster (no network latency)\n",
    "- âœ… Full control over the model\n",
    "\n",
    "**When to use HuggingFace Hub:**\n",
    "- Need access to specific models not available in Ollama\n",
    "- Want to try many different models quickly\n",
    "- Don't have local hardware resources\n",
    "- Prefer cloud-based solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793378",
   "metadata": {},
   "source": [
    "## 8. Text Summarization\n",
    "\n",
    "Use LLMs to summarize long texts concisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a summarization prompt\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Please summarize the following text in 2-3 sentences:\\n\\n{text}\\n\\nSummary:\"\n",
    ")\n",
    "\n",
    "# Create summarization chain\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Long text to summarize\n",
    "long_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that focuses on the development \n",
    "of algorithms and statistical models that enable computers to improve their performance \n",
    "on tasks through experience. Unlike traditional programming where explicit instructions \n",
    "are provided, machine learning systems learn patterns from data. There are three main types \n",
    "of machine learning: supervised learning (learning from labeled data), unsupervised learning \n",
    "(finding patterns in unlabeled data), and reinforcement learning (learning through interaction \n",
    "with an environment). Machine learning has revolutionized various industries including healthcare, \n",
    "finance, transportation, and entertainment, enabling applications like medical diagnosis, \n",
    "fraud detection, autonomous vehicles, and personalized recommendations.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_chain.invoke({\"text\": long_text})\n",
    "print(\"Original Text Length:\", len(long_text), \"characters\")\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9754f",
   "metadata": {},
   "source": [
    "## 9. Text Translation\n",
    "\n",
    "Translate text between different languages using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation prompt\n",
    "translate_prompt = PromptTemplate(\n",
    "    input_variables=[\"source_lang\", \"target_lang\", \"text\"],\n",
    "    template=\"Translate the following text from {source_lang} to {target_lang}:\\n\\n{text}\\n\\nTranslation:\"\n",
    ")\n",
    "\n",
    "# Create translation chain\n",
    "translate_chain = translate_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Translate text\n",
    "translations = [\n",
    "    {\"source_lang\": \"English\", \"target_lang\": \"Spanish\", \"text\": \"Hello, how are you today?\"},\n",
    "    {\"source_lang\": \"English\", \"target_lang\": \"French\", \"text\": \"Machine learning is transforming the world.\"},\n",
    "    {\"source_lang\": \"English\", \"target_lang\": \"German\", \"text\": \"I love learning new programming languages.\"}\n",
    "]\n",
    "\n",
    "for translation_request in translations:\n",
    "    result = translate_chain.invoke(translation_request)\n",
    "    print(f\"{translation_request['source_lang']} â†’ {translation_request['target_lang']}\")\n",
    "    print(f\"Original: {translation_request['text']}\")\n",
    "    print(f\"Translation: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58012dca",
   "metadata": {},
   "source": [
    "## 10. Question Answering with Context\n",
    "\n",
    "Provide context to help the model answer questions more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a QA prompt with context\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Use the following context to answer the question. \n",
    "If you don't know the answer, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Example QA\n",
    "context = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "It enables applications that are data-aware and agentic. LangChain provides tools for:\n",
    "1. Connecting to various data sources\n",
    "2. Building chains of language model calls\n",
    "3. Creating agents that can take actions\n",
    "4. Managing memory and conversation history\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"What are the main tools provided by LangChain?\",\n",
    "    \"Can LangChain create agents?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_chain.invoke({\"context\": context, \"question\": question})\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ebfe6",
   "metadata": {},
   "source": [
    "## 11. Temperature and Randomness\n",
    "\n",
    "Temperature controls the randomness of the model's output. Lower temperatures make output more deterministic, while higher temperatures make it more creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Low temperature (deterministic/focused)\n",
    "llm_low_temp = Ollama(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=OLLAMA_HOST,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# High temperature (creative/varied)\n",
    "llm_high_temp = Ollama(\n",
    "    model=\"qwen3:4b\",\n",
    "    base_url=OLLAMA_HOST,\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "prompt = \"Write a creative tagline for an eco-friendly fashion brand.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOW TEMPERATURE (0.1) - More Deterministic:\")\n",
    "print(\"=\" * 60)\n",
    "response_low = llm_low_temp.invoke(prompt)\n",
    "print(response_low)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HIGH TEMPERATURE (0.9) - More Creative:\")\n",
    "print(\"=\" * 60)\n",
    "response_high = llm_high_temp.invoke(prompt)\n",
    "print(response_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaadb72",
   "metadata": {},
   "source": [
    "## 12. Output Parsing\n",
    "\n",
    "Parse structured outputs from LLMs to extract specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Create an output parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Get format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt with format instructions\n",
    "list_prompt = PromptTemplate(\n",
    "    template=\"List 5 {item_type}.\\n{format_instructions}\",\n",
    "    input_variables=[\"item_type\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "# Create chain with parser\n",
    "list_chain = list_prompt | llm | output_parser\n",
    "\n",
    "# Get a structured list\n",
    "items = list_chain.invoke({\"item_type\": \"popular programming languages\"})\n",
    "print(\"Type of output:\", type(items))\n",
    "print(\"Parsed items:\", items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cf903",
   "metadata": {},
   "source": [
    "## 13. Chat Prompts and Messages\n",
    "\n",
    "Chat prompts allow for more structured conversation-like interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10032f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# Create a chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a helpful AI assistant that specializes in {specialty}. \"\n",
    "        \"Answer questions clearly and concisely.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "])\n",
    "\n",
    "# Create chat chain\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chat prompt\n",
    "response = chat_chain.invoke({\n",
    "    \"specialty\": \"Python programming\",\n",
    "    \"user_input\": \"What are decorators in Python and why are they useful?\"\n",
    "})\n",
    "\n",
    "print(\"Assistant Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24f812",
   "metadata": {},
   "source": [
    "## 14. Conversation Memory\n",
    "\n",
    "Maintain context across multiple interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Create a memory object\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Have a multi-turn conversation\n",
    "responses = []\n",
    "responses.append(conversation.predict(input=\"Hi, my name is Alice\"))\n",
    "responses.append(conversation.predict(input=\"What is 25 + 17?\"))\n",
    "responses.append(conversation.predict(input=\"Can you remember my name?\"))\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    print(f\"Turn {i}: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931e3ac",
   "metadata": {},
   "source": [
    "## 15. Common Use Cases and Limitations\n",
    "\n",
    "### Common Use Cases:\n",
    "1. **Content Generation**: Blog posts, product descriptions, creative writing\n",
    "2. **Question Answering**: Customer support, knowledge retrieval\n",
    "3. **Summarization**: News summaries, document condensing\n",
    "4. **Translation**: Multi-language support\n",
    "5. **Code Generation**: Assisting in programming tasks\n",
    "6. **Sentiment Analysis**: Understanding text emotion\n",
    "7. **Information Extraction**: Pulling data from text\n",
    "\n",
    "### Limitations:\n",
    "1. **Hallucination**: Can generate plausible-sounding but false information\n",
    "2. **Knowledge Cutoff**: Only trained on data up to a certain date\n",
    "3. **Reasoning**: Struggles with complex logical reasoning\n",
    "4. **Bias**: May reflect biases present in training data\n",
    "5. **Context Length**: Limited by maximum token length\n",
    "6. **Real-time Information**: Cannot access current information\n",
    "7. **Explainability**: Difficult to understand reasoning behind decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548e4b4",
   "metadata": {},
   "source": [
    "## 16. Best Practices for Using LLMs\n",
    "\n",
    "### 1. Prompt Engineering\n",
    "- Be specific and clear in your prompts\n",
    "- Provide examples (few-shot learning)\n",
    "- Use system prompts to set context\n",
    "- Experiment with different phrasings\n",
    "\n",
    "### 2. Temperature Settings\n",
    "- Use low temperature (0.1-0.3) for consistent, factual tasks\n",
    "- Use medium temperature (0.5-0.7) for balanced tasks\n",
    "- Use high temperature (0.8-1.0) for creative tasks\n",
    "\n",
    "### 3. Error Handling\n",
    "- Always validate LLM outputs\n",
    "- Implement fallback mechanisms\n",
    "- Use output parsers to structure responses\n",
    "- Monitor for hallucinations\n",
    "\n",
    "### 4. Performance Optimization\n",
    "- Cache common responses\n",
    "- Batch process when possible\n",
    "- Use shorter contexts when applicable\n",
    "- Monitor token usage\n",
    "\n",
    "### 5. Ethical Considerations\n",
    "- Be aware of potential biases\n",
    "- Avoid using LLMs for sensitive decision-making\n",
    "- Disclose AI usage to users\n",
    "- Protect user data and privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2803dc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive introduction to LLMs, we covered:\n",
    "\n",
    "1. **What are LLMs** - Understanding the fundamentals\n",
    "2. **Setup** - Installing and initializing Ollama\n",
    "3. **Direct Invocation** - Using LLMs directly\n",
    "4. **Tokens** - Understanding token counting and usage\n",
    "5. **Prompts** - Working with prompt templates\n",
    "6. **Few-shot Learning** - Teaching by example\n",
    "7. **Chaining** - Combining prompts and LLMs\n",
    "8. **Sequential Chains** - Building complex workflows\n",
    "9. **Summarization** - Condensing long texts\n",
    "10. **Translation** - Converting between languages\n",
    "11. **QA Systems** - Context-aware question answering\n",
    "12. **Temperature Control** - Managing randomness\n",
    "13. **Output Parsing** - Structuring LLM outputs\n",
    "14. **Chat Prompts** - Conversation-style interactions\n",
    "15. **Memory** - Maintaining conversation context\n",
    "16. **Use Cases & Limitations** - Practical understanding\n",
    "17. **Best Practices** - Tips for effective LLM usage\n",
    "\n",
    "### Key Takeaway\n",
    "All examples in this notebook use **Ollama (qwen3:4b)** - a free, local, open-source LLM that requires no API keys and keeps your data private!\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different prompts and temperatures\n",
    "- Build chains for specific use cases\n",
    "- Integrate with external data sources\n",
    "- Explore vector databases for semantic search\n",
    "- Create sophisticated multi-step applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
