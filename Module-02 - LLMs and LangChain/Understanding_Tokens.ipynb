{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02913534",
   "metadata": {},
   "source": [
    "Tokenization strategies:\n",
    "1. Character Level: Treat every character as a token.\n",
    "2. Word Level: Split on whitespace/punctuation.\n",
    "3. Subword Level (BPE/WordPiece): Merge frequent pairs to balance vocabulary size and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453d8ee",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "\"This is tokenizing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c7a6d",
   "metadata": {},
   "source": [
    "**Character Level**\n",
    "[T][h][i][s][ ][i][s][ ][t][o][k][e][n][i][z][i][n][g][.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d6cd1",
   "metadata": {},
   "source": [
    "**Word Level**\n",
    "[This][is][tokenizing][.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238507c",
   "metadata": {},
   "source": [
    "**Subword Level (BPE-style)**\n",
    "[This][is][token][izing][.]\n",
    "Subword encoding reduces vocab size and handles rare/novel words by merging frequent character pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335ae02",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "BPE builds a vocabulary by iteratively merging the most frequent symbol pairs. It balances expressiveness (fewer unknowns) with manageable vocab size. Many modern LLMs use a BPE variant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdea724",
   "metadata": {},
   "source": [
    "### Tokens and Cost\n",
    "LLM usage often scales with tokens (prompt + completion). Rough guide: ~1K tokens ≈ 750 words. Staying within model limits avoids truncation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27408665",
   "metadata": {},
   "source": [
    "### Tokenizers in Action (Hugging Face)\n",
    "Below we load a pretrained tokenizer (GPT-2) and inspect how it encodes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e89b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad16982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willis\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Willis\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Willis\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'Ġis', 'Ġa', 'Ġsample', 'Ġtext', 'Ġto', 'Ġtest', 'Ġthe', 'Ġtoken', 'izer', '.']\n",
      "Token IDs: [1212, 318, 257, 6291, 2420, 284, 1332, 262, 11241, 7509, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer (uses BPE)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "sample_text = 'This is a sample text to test the tokenizer.'\n",
    "\n",
    "# Encode to token IDs\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "print('Tokens:', tokens)\n",
    "print('Token IDs:', token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda6900",
   "metadata": {},
   "source": [
    "### Why do tokens show the special `Ġ` character?\n",
    "\n",
    "Many GPT‑2–style, byte‑level BPE tokenizers (used in popular LLMs and in the `gpt2` tokenizer we demo here) mark leading whitespace with a visible symbol in the token string representation:\n",
    "\n",
    "- `Ġ`: indicates a preceding space before the token. For example, the text \" tokenizer\" becomes the tokens `['Ġtoken', 'izer']`. The `Ġ` is not literally in your text; it annotates that a space existed before `token`.\n",
    "- `Ċ`: represents a newline character (`\\n`). For example, \"Hello\\nWorld\" can tokenize to something like `['Hello', 'Ċ', 'World']`.\n",
    "\n",
    "These markers make whitespace explicit during tokenization so that the tokenizer can faithfully round‑trip between text and tokens. When you decode token IDs back to text, the tokenizer uses these markers to reconstruct spaces and newlines correctly.\n",
    "\n",
    "Practical tips:\n",
    "- Token counts include these markers. Changing spaces or newlines can change your token count and chunk boundaries.\n",
    "- When comparing tokens across chunks, remember that leading spaces produce `Ġ...` tokens; removing/adding a space can change how a word splits.\n",
    "- The markers are an implementation detail of the tokenizer; your original text does not contain `Ġ` or `Ċ`, and decoding removes them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f0bcd",
   "metadata": {},
   "source": [
    "### Tokenizer Considerations\n",
    "- Uppercase/lowercase: same word may map to different tokens.\n",
    "- Numbers: may split into multiple tokens.\n",
    "- Whitespace: trailing spaces can change tokens.\n",
    "- Model-specific: tokenizers differ across models (GPT, LLaMA, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376ab65",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You now understand what tokens are, how common tokenizers behave, and why token limits matter. Next, we will explore chains with LangChain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
