{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c573c18",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Large Language Models (LLMs) generate text from prompts. Chat Models handle structured multi-message interactions (system, user, etc.) and remember session context. We'll see both using Gemini via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc1e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama setup: ensure 'ollama serve' is running in a terminal\n"
     ]
    }
   ],
   "source": [
    "# Setup: install dependencies (run if needed)\n",
    "# Hint: uncomment the pip line below the first time.\n",
    "# %pip install -q langchain langchain-community ollama python-dotenv\n",
    "\n",
    "# Before running: ensure Ollama is installed and running locally\n",
    "# Download from https://ollama.ai\n",
    "# Pull qwen2:4b: ollama pull qwen2:4b\n",
    "# Start server: ollama serve (runs on http://localhost:11434 by default)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(\"Ollama setup: ensure 'ollama serve' is running in a terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860142d4",
   "metadata": {},
   "source": [
    "## Setup: Ollama Local Models\n",
    "This notebook uses **Ollama** for free, local inference.\n",
    "\n",
    "1. **Install Ollama**: https://ollama.ai\n",
    "2. **Pull the model**: Open a terminal and run:\n",
    "   ```bash\n",
    "   ollama pull qwen2:4b\n",
    "   ```\n",
    "3. **Start Ollama server** (in a separate terminal):\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "   It will run on `http://localhost:11434` by default.\n",
    "4. **Run this notebook**: Cells below will automatically connect to the local server.\n",
    "\n",
    "No API keys, no quotas, no costsâ€”everything runs on your machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa99c1",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "LLMs take a single prompt string and return text. Great for generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0eb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 short, catchy, and brand-friendly product names for wireless headphonesâ€”**all under 3 syllables** and designed to hint at wireless freedom, clarity, or modern tech without being generic:\n",
      "\n",
      "1. **Nex**  \n",
      "   *(Short, tech-forward, implies \"connection\" and \"next-level\" soundâ€”easy to remember and trademark-friendly)*  \n",
      "\n",
      "2. **Lume**  \n",
      "   *(Evokes \"light\" and \"clarity\"â€”suggests crisp audio and wireless freedom, with a modern, minimalist feel)*  \n",
      "\n",
      "3. **Flow**  \n",
      "   *(Simple, intuitive, and directly ties to seamless wireless listeningâ€”perfect for a smooth user experience)*  \n",
      "\n",
      "### Why these work:\n",
      "- **Ultra-short**: All 1â€“2 syllables (ideal for branding, social media, and global appeal).  \n",
      "- **Wireless hint**: No literal words like \"wireless\" or \"free\" (avoids sounding generic), but implies connection, fluidity, or clarity.  \n",
      "- **Memorable**: Easy to spell, say, and brand (e.g., \"Nex Headphones\" vs. \"Wireless Headphones\").  \n",
      "- **No trademark conflicts**: These names are not widely used in the headphone space (verified via quick check).  \n",
      "\n",
      "**Top pick**: **Nex**â€”itâ€™s the most versatile, tech-savvy, and instantly communicates *modern wireless innovation* without overcomplicating.  \n",
      "\n",
      "*Example*: \"Nex: Wireless headphones that connect you to the world, without wires.\"  \n",
      "\n",
      "Let me know if you'd like more options tailored to a specific vibe (e.g., premium, playful, minimalist)! ðŸŽ§\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model_name = \"qwen3:4b\"\n",
    "temperature = 0.1   #low temp for more deterministic output\n",
    "llm = Ollama(model=model_name, temperature=temperature)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Write a short product name for a company that makes {product}.\"\n",
    ")\n",
    "\n",
    "# Use pipe operator instead of LLMChain\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"product\": \"wireless headphones\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2fdb5",
   "metadata": {},
   "source": [
    "### Sample Output\n",
    "A short product name like \"Circless Headphones\" (your result will vary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd3a29",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "Chat models process a list of messages (system, user, etc.). Ideal for assistants and multi-turn contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9957d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime la programmation.\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Build a simple translator with Ollama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chat_model_name = model_name or \"qwen3:4b\"  \n",
    "llm = Ollama(model=chat_model_name, temperature=temperature)\n",
    "\n",
    "# TODO: customize your translation prompt and language\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Translate the following English text to French:\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt.format(text=\"I love programming.\"))\n",
    "print(response)  # Expect a French translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f768f6a4",
   "metadata": {},
   "source": [
    "## API Differences in LangChain\n",
    "- **LLMs:** single prompt in, text out; great for generation.\n",
    "- **Chat Models:** list of messages; support roles (system/user) and multi-turn context.\n",
    "- **Prompting:** `PromptTemplate` for LLMs; message objects for Chat Models.\n",
    "- **Use Cases:** LLMs for one-shot tasks; Chat for assistants and dialogues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc0608",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "LLMs and Chat Models both use Gemini in LangChain; pick based on task shape: single-shot generation vs multi-message dialogue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
